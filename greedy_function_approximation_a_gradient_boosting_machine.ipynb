{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21d290f-8915-4268-9f9b-0fdf28aa1fed",
   "metadata": {},
   "source": [
    "# PAPERS DECONSTRUCTED\n",
    "## Title: Greedy Function Approximation: A Gradient Boosting Machine\n",
    "### Authors: Jerome H. Friedman\n",
    "Link: https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full\n",
    "\n",
    "*Papers deconstructed* are a result of my frustration when reading papers and walking away with a fuzzy understanding. By folding in commentary, Python code implementations, I aim to make the paper much more understandstable to all. I also find it really fun :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172d037-6bec-47b9-bcba-d0c744e7089f",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "Most machine learning algorithms by finding the best parameter values for a loss function and training dataset. Gradient descent is typically used to solve this. This works (roughly) as follows:\n",
    "\n",
    "(1) Fetch the true value for the given training example\n",
    "\n",
    "(2) Compute a predicted value for the given training example\n",
    "\n",
    "(3) Compute the loss given the true value and predicted value\n",
    "\n",
    "(4) Compute the gradient of the loss with respect to the parameters\n",
    "\n",
    "In this paper step (2) is left as an unknown and the gradient is _directly_ learned from data instead. This is done by setting up a supervised machine learing problem of the following:\n",
    "\n",
    "Training Dataset = $\\{ (\\tilde{y_i}, x_i) \\}_i^N$ \n",
    "\n",
    "where $\\tilde{y_i}$ is the gradient of the loss with respect to the prediction value evaluated at the _previous_ prediction value. Learning the gradient is repeated at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79267e60-984d-458b-84fe-a399ffe0b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ab196-ebea-4389-85f8-2e58ea04cf1e",
   "metadata": {},
   "source": [
    "# Full Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebfdb6-ead1-4e5f-a27e-d26c7d41f790",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16513821-f8d9-40df-a23f-038cb12b2dd0",
   "metadata": {},
   "source": [
    "> <i>Function estimation/approximation is viewed from the perspective of\n",
    "numerical optimization in function space, rather than parameter space</i>\n",
    "\n",
    "Instead of optimizing for a parameter value (like the slope value in linear regression), we are optimizing for a function directly. This is unusual. ML algorithms are typically designed to optimize over parameter values, not functions directly. We typically calculate something like $\\frac{\\partial J}{\\partial \\theta}$ (where J is the cost function and $\\theta$ is a parameter value) but here we are calculating something like $\\frac{\\partial J}{\\partial f(x)}$. We take a derivative with respect to a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b952c-4692-488e-9c0d-3456f40c5b4b",
   "metadata": {},
   "source": [
    "> <i>A connection is made between stagewise additive expansions and steepest descent minimization.</i>\n",
    "\n",
    "_steepest-descent minimization_ is not the same as _gradient descent_. See this <a href=\"https://stats.stackexchange.com/a/322177/30545\">link</a>. The link also includes a <a href=\"http://www.math.usm.edu/math/lambers/mat419/lecture10.pdf\">reference</a> that's helpful. Steepest-descent minimization is a special case of gradient descent. Gradient descent is using knowledge of the gradient to choose where to step next. But _how_ you take a step, leads you to specific algorithms (such as steepest-descent).\n",
    "\n",
    "In steepest-descent, the goal is to find a $\\alpha$ that minimizes $g$:\n",
    "\n",
    "$g({\\alpha}) = f(x_{t-1} - \\alpha \\nabla f(x_{t-1})) \\;\\;\\; \\alpha \\ge 0$\n",
    "\n",
    "I'm not a numerical specialist, but there's a tradeoff here. While we are exact (as opposed to using a fixed step size), this requires solving an additional optimization problem at each step. I would think the tradeoffs are problem specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb7008-b913-4ebb-83bb-372fc0607a89",
   "metadata": {},
   "source": [
    "> <i>A general gradient descent “boosting” paradigm is\n",
    "developed for additive expansions based on any fitting criterion.</i>\n",
    "\n",
    "Additive models are models where can break the model into a sum of models:\n",
    "\n",
    "f(x) = $\\sum_i f_i(x)$\n",
    "\n",
    "Stagewise (as opposed to _stepwise_) means we can't go back and edit. Each 'stage' freezes all previous terms. Section **3.3.3 Forward-Stagewise Regression** <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">The Elements of Statistical Learning</a> is helpful.\n",
    "\n",
    "The boosting paradigm mentioned is a reference to Freund and Schapire's <a href=\"https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf\">paper</a> in the late 90s that introduced the AdaBoost algorithm. The idea of boosting is to build a many models in a sequence. Each model learns from the mistakes from the previous model.\n",
    "\n",
    "By \"any fitting criterion\" means that this works (essentially) for any loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab1dfc-2da5-4fd2-89ea-67fa7d3d63de",
   "metadata": {},
   "source": [
    "> <i>Specific algorithms are presented for least-squares, least absolute deviation, and\n",
    "Huber-M loss functions for regression, and multiclass logistic likelihood\n",
    "for classification</i>\n",
    "\n",
    "Let's show what each of these losses look like mathematically and implemented in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebcdcc4-a0ef-4a90-9b6c-66cd88f491dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we setup some data to demonstrate each function with\n",
    "\n",
    "# True function\n",
    "f_star = lambda x: x**2\n",
    "\n",
    "# Estimated function\n",
    "# (we introduce a little bit of error)\n",
    "f = lambda x: (x + 0.1)**2\n",
    "\n",
    "# Points to evaluate over\n",
    "x = np.arange(0, .5, step=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae5fd2-a28f-4760-ae93-4b73ca1ceeb4",
   "metadata": {},
   "source": [
    "**least-squares loss**\n",
    "\n",
    "$l(y, f) = \\frac{1}{2} (y - f)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28181974-64b4-4e2e-b2c6-0a8a038c277e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00e-05, 2.00e-04, 4.50e-04, 8.00e-04, 1.25e-03, 1.80e-03,\n",
       "       2.45e-03, 3.20e-03, 4.05e-03, 5.00e-03])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def least_squares(y_true, y_pred):\n",
    "    loss = .5 * (y_true - y_pred)**2\n",
    "    return loss\n",
    "\n",
    "least_squares(f_star(x), f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef921bf-ace0-44e5-9d58-bc1bb07fa3fc",
   "metadata": {},
   "source": [
    "**least-absolute deviation**\n",
    "\n",
    "$l(y, f) = |y - f|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9664703-59b6-44ca-b8db-90e043128314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def least_absolute_deviation(y_true, y_pred):\n",
    "    loss = np.abs(y_true - y_pred)\n",
    "    return loss\n",
    "\n",
    "least_absolute_deviation(f_star(x), f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06a428-c0f7-4959-9804-8b96e43bfd23",
   "metadata": {},
   "source": [
    "**huber-m**\n",
    "\n",
    "if $|y - f| \\le \\delta$ then:\n",
    "\n",
    "$l(y, f) =  \\frac{1}{2} (y - f)^2$\n",
    "\n",
    "else:\n",
    "\n",
    "$l(y, f) =  \\delta(|y - f| - \\frac{\\delta}{2})$\n",
    "\n",
    "This is the first time I had across Huber loss. The paper can be found <a href=\"https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full\">here</a>.\n",
    "\n",
    "The \"m\" I believe comes from the term \"m-estimator\". I _think_ the m means \"maximum likelihood type\" estimator. See <a href=\"https://www.statisticalconsultants.co.nz/blog/m-estimators.html\">here</a>. It comes from robust statistical methods. Something I myself would need to learn more about. This function acts as a switch. When the error gets too high it acts as a switch to a loss that doesn't penalize as heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dd21bf46-c616-4005-aff9-34604511aa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00e-05, 2.00e-04, 4.50e-04, 8.00e-04, 1.25e-03, 1.80e-03,\n",
       "       2.45e-03, 3.20e-03, 4.05e-03, 5.00e-03])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def huber(y_true, y_pred, delta=5.):\n",
    "    z = y_true - y_pred\n",
    "    z_abs = np.abs(z)\n",
    "    loss = np.where(z_abs <= delta, .5 * z**2, delta * (z_abs - delta/2.))\n",
    "    return loss\n",
    "\n",
    "huber(f_star(x), f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb4459-c304-4bd0-a34d-440332f95ea7",
   "metadata": {},
   "source": [
    "**multi-class logistic likelihood**\n",
    "\n",
    "$l(y, f) = - \\sum_k^{\\text{n_classes}} y_k \\text{log} \\, p_k$\n",
    "\n",
    "The variable k is to iterate over each class. $y_k$ is either 0/1 and $p_k$ is your models outputted \"probability\" for that class. I put the probability in quotes because you don't always get good probabiity estimates in practice.\n",
    "\n",
    "We can write down the special case of a binary label (0/1):\n",
    "\n",
    "$l(y, f) = -  \\big( y_k \\text{log} \\, p_k + (1 - y_k) \\text{log} \\, (1 - p_k) \\big)$\n",
    "\n",
    "This acts as a switch. When the label is 1 we get:\n",
    "\n",
    "$l(y, f) = - \\text{log} \\, p_k$\n",
    "\n",
    "When the label is 0 we get:\n",
    "\n",
    "$l(y, f) = - \\text{log} \\, (1 - p_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d4b82bb-9b56-4ae5-8b87-5843f929bfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69815968, 0.68196046, 0.67334717, 0.72488538, 0.73915934,\n",
       "       0.75627179, 0.77634377, 0.79951423, 0.57593942, 0.55329211])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logistic_likelihood(y_true, y_pred, delta):\n",
    "    loss = -(y_true * np.log(y_pred) + (1. - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "y_true = np.random.randint(0, 2, len(x))\n",
    "\n",
    "# Pass through sigmoid to get a number\n",
    "# between 0 and 1. Acts like a probability\n",
    "y_pred = sigmoid(f(x))\n",
    "\n",
    "logistic_likelihood(y_true, sigmoid(f(x)), delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ed08d-c1e2-47b9-9024-164ad28e1872",
   "metadata": {},
   "source": [
    "This is a function we use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee546ec-5dfc-470f-aea8-3448feacd31a",
   "metadata": {},
   "source": [
    "> <i>Special enhancements are derived for the particular case\n",
    "where the individual additive components are regression trees, and tools\n",
    "for interpreting such “TreeBoost” models are presented.</i>\n",
    "\n",
    "They exploit the special case when the additive model is comprised of decision trees--which they then call \"TreeBoost\". Note that <a href=\"https://xgboost.readthedocs.io/en/stable/\">XGBoost</a> by default uses TreeBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793c429-3d6d-490c-9de0-68a1376e4611",
   "metadata": {},
   "source": [
    "> <i>Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for\n",
    "mining less than clean data. Connections between this approach and the\n",
    "boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.</i>\n",
    "\n",
    "Lastly, they mention how all this ties in with the Adaboost algorithm (which introduced the idea of boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524c49b-e99b-428b-b1d4-db7c1c5c03c0",
   "metadata": {},
   "source": [
    "# 1 Function Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ee195-4d0c-4839-a113-f4b1fb917367",
   "metadata": {},
   "source": [
    "> <i>In the function estimation or \"predictive learning\" problem, one has a system consisting of a random \"output\" or \"response\" variable y and a set of random \"input\" or \"explanatory\" variables $\\bf{x} = \\{x_1, ..., x_n\\}$</i>\n",
    "\n",
    "You are given labeled data (i.e., training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc69fe-e621-4c48-a4f6-e1da3be7b5b5",
   "metadata": {},
   "source": [
    "><i>Using a “training” sample $\\{y_i, \\bf{x_i}\\}_{i}^{N}$\n",
    "1 of known $(y, \\bf{x})$-values, the goal is to\n",
    "obtain an estimate or approximation $\\hat{F}(\\bf{x})$, of the function $F^{*}(x)$ mapping $\\bf{x}$ to y, that minimizes the expected value of some specified loss function\n",
    "$L(y, F(\\bf{x}))$ over the joint distribution of all ($y, \\bf{x})$-values,\n",
    "<br/>\n",
    "<br/>\n",
    "$F^* = \\text{argmin}_F E_{y, \\bf{x}} L(y, F(\\bf{x})) = \\text{argmin}_F E_x [E_y(L(y, F(\\bf{x}))) | \\bf{x}]$\n",
    "</i>\n",
    "\n",
    "$F^{*}(x)$ is the true function. Unknown in practice (unless you are simulating data). But we can observe how inputs are relate to the outputs via labeled data. $F(x)$ is the function we learn from data. But to do learning, we need a measure of how good we are doing. Hence, the loss function:\n",
    "\n",
    "$L(y, F(x)) = L(F^{*}(x), F(x))$.\n",
    "\n",
    "A smaller loss value is better--less prediction error. We must compute the loss over each $(\\vec{x},y)$ pair. When the losses can be computed in parallel, we call that a univariate loss function. The opposite of this are multivariate losses. For example, see work done by Thorsten <a href=\"https://www.cs.cornell.edu/people/tj/publications/joachims_05a.pdf\">here</a>. Multivariate losses are used (for example) when designing a ranking algorithm.\n",
    "\n",
    "We want to summarize all these losses. A way to do this is to take a sum or an average. Typically the average loss is used.\n",
    "\n",
    "To get the form you see in this paper, I actually answered this many years ago <a href=\"https://math.stackexchange.com/a/623473/118474\">here</a>. But here it is again:\n",
    "\n",
    "$\\text{Expected Loss} = \\int_x \\int_y L(y, F(x)) P(x, y) \\, dx dy$\n",
    "\n",
    "By Bayes' theorem: $P(x, y) = P(x) P(y | x)$\n",
    "\n",
    "Let's substitute\n",
    "\n",
    "$\\int_x \\int_y L(y, F(x))  P(x) P(y | x) \\, dx dy$\n",
    "\n",
    "We can rearrange:\n",
    "\n",
    "$\\int_x P(x) \\big( \\int_y L(y, F(x)) P(y | x) \\, dy \\big) dx$\n",
    "\n",
    "And $\\int_y L(y, F(x)) P(y | x) dy$ is by definition $E_y [L(y, F(x) | x]$\n",
    "\n",
    "so \n",
    "\n",
    "$\\int_x P(x) \\big( E_y [L(y, F(x) | x] \\big) dx$\n",
    "\n",
    "But this is again, another expectation. So now we get:\n",
    "\n",
    "$E_x[E_y [L(y, F(x) | x]]$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$F^* = {\\text{argmin}}_F \\, E_{xy}[L(y, F(x))] = {\\text{argmin}}_F \\, \\int_x \\int_y P(x, y) L(y, F(x)) \\, dx dy$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9d1a0-628e-4cc5-8f37-fe2c69e26534",
   "metadata": {},
   "source": [
    "> <i>Frequently employed loss functions $L(y, F)$ include squared-error $(y − F^2)$ and absolute error $|y − F|$ for $y \\in R^1$ (regression) and negative binomial loglikelihood, $log(1 + e^{−2yF}$, when $y \\in \\{−1, 1\\}$ (classification).</i>\n",
    "\n",
    "This is telling us specific cases of what $L(y, F(X))$ can be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c0ab8-112b-4fe7-977f-0fd600d346fd",
   "metadata": {},
   "source": [
    "> <i>A common procedure is to restrict $F(\\bf{x})$ to be a member of a parameterized class of functions $F(\\bf{x}; \\bf{P})$, where $P = \\{P_1, P_2, ...\\}$ is a finite set of parameters whose joint values identify individual class members.</i>\n",
    "\n",
    "I will give a concrete example here. Take simple linear regression. Its takes the form of:\n",
    "\n",
    "$F(x) = b + mx$\n",
    "\n",
    "To rewrite in the notation just given:\n",
    "\n",
    "$F(x; \\bf{P} ) = p_1 + p_2 x$\n",
    "\n",
    "So $P = \\{p_1, p_2\\}$. Note that we fixed the function (basically) and now seek what the values of $b$ and $m$ are. I say basically because technically, each new set of parameters map out a different function, but the models _structure_ remains the same (i.e., linear form)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab351a-5815-4cda-ba13-9c3977be48e7",
   "metadata": {},
   "source": [
    "> <i>In this article we focus on \"additive\" expansions of the form $F(\\bf{x}; \\{\\beta_m, \\bf{a}_m\\}_1^M) = \\sum_m \\beta_m h(\\bf{x}; \\bf{a}_m)$</i>\n",
    "\n",
    "This additive expansion is saying, we have $m$ models that get added together. Each multiplied by a weight of $\\beta_m$. (The $m$ denotes which model we are referring to--there are $m$ of them). Each model is denoted as $h(x; a_m)$. The $a_m$ is telling us each model itself has a set of parameters that need to be estimated from data. And these models do not share parameters (would be interesting to think of use-cases where parameter sharing might be beneficial). Also, I see no reason why each model must keep the same structural form (decision tree vs naive bayes). But in practice, and even in this paper, $h(x; a_m)$ is left to be a decision tree. In the package XGBoost for example, $h(x; a_m)$ is referred to as the _booster_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f634b4-df4e-45eb-a775-b4d77fe9774d",
   "metadata": {},
   "source": [
    "> <i>The (generic) function $h(\\bf{x}, a)$ in (2) is usually a simple parameterized function of the input variables $\\bf{x}$, characterized by parameters $\\bf{a} = \\{a_1, a_2, ... \\}$. The individual terms differ in the joint values $a_m$ chosen for these parameters.</i>\n",
    "\n",
    "Yep. Each model will estimate $a_m$ to have different values than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c4ffa-f9ec-42ec-a25f-11b52ba049b1",
   "metadata": {},
   "source": [
    ">  <i>Such expansions (2) are at the heart of many function approximation methods such as neural networks, radial basis functions, wavelets, and support vector machines. Of special interest here is the case where each of the functions $h(\\bf{x}; \\bf{a}_m)$ is a small regression tree, such as those produced in CART. For a regression tree the parameters $\\bf{a}_m$ are the splitting variables, split locations, and the terminal node means of the individual trees.</i>\n",
    "\n",
    "They mention a bunch of ML models where this type of additive model is applicable. But the one of interest here, is the regression tree (i.e., decision tree). To parameterize this tree, at each node we have to choose a variable and a split point for that variable. So here they are saying that $a_m$ will denote the feature chosen for each node and its split point. We can count the number of parameters. At depth $l$ we will have $2^l$ nodes. So there are:\n",
    "\n",
    "$\\text{number of decision nodes} = \\sum_l^{D} 2^l$ where $D$ is the max depth\n",
    "\n",
    "$\\sum_l^{D} 2^l = 2^{D + 1} - 1$ (see <a href=\"https://math.stackexchange.com/a/1990146/118474\">proof</a>)\n",
    "\n",
    "And two parameters for each node (which variable and its split point):\n",
    "\n",
    "$ 2 * (2^{D + 1} - 1) = 2^{D + 2} - 2$\n",
    "\n",
    "I've built models in practice with depths reaching over 10 (even up to 15). So with $D = 15$ we get $131070$ parameters for a single decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04ed3dd-4f48-4c4f-90e6-cc1ebecd91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131070"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_params(max_depth):\n",
    "    n_param = (2**(max_depth + 2) - 2)\n",
    "    n_param_from_longer_calculation = 2 * sum([2**j for j in range(max_depth + 1)])\n",
    "    \n",
    "    # Evidence for proof\n",
    "    assert n_param == n_param_from_longer_calculation\n",
    "    \n",
    "    return n_param\n",
    "\n",
    "get_num_params(max_depth=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b16207-39a8-431b-a1ac-8570e39d604d",
   "metadata": {},
   "source": [
    "# 1.1 Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d354515-bb52-48cb-8f9b-9a7542f93245",
   "metadata": {},
   "source": [
    "> <i>In general, choosing a parameterized model $F(\\bf{x}; \\bf{P})$ changes the function optimization problem to one of parameter optimization\n",
    "<br />\n",
    "<br />\n",
    "$P^* = \\text{argmin}_P \\Phi(\\bf{P})$\n",
    "<br />\n",
    "<br />\n",
    "where\n",
    "<br />\n",
    "<br />\n",
    "$ \\Phi(P) = E_{y, x} L(y, F(x; P))$\n",
    "<br />\n",
    "<br />\n",
    "and then\n",
    "<br />\n",
    "<br />\n",
    "$ F^*(x) = F(x; P^*)$</i>\n",
    "\n",
    "Yes. In linear regression (for example) we use the loss function of least squares to find the best set of parameters. But the key here is that we fix the structure of the model and optimize over the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9c338-6e1d-4538-a647-d229e224612f",
   "metadata": {},
   "source": [
    "> <i>For most $F(x; P)$ and L, numerical optimization methods must be applied to solve $P^* = \\text{argmin}_P \\Phi(\\bf{P})$. This often involves expression the solution for the parameters in the form\n",
    "<br />\n",
    "<br />\n",
    "$P^* = \\sum_{m=0}^M p_m$\n",
    "<br />\n",
    "<br />\n",
    "where $p_0$ is the an initial guess and $\\{ p_m\\}_1^M $ are successive increments (\"steps\" or \"boosts\"), each based on the sequence of preceding steps. The precription for computing each step $p_m$ is defined by the optimization method.</i>\n",
    "\n",
    "in neural networks, we leverage gradient descent. The weight parameters get updated at each step by adding a value to them. Sp the final weights learned are a sum of the weight updates from each step. That is: \n",
    "\n",
    "$\\vec{w}_{final} = \\sum_i w_i$\n",
    "\n",
    "We start with an initial guess of parameters ($w_0$) and increment after each step. In the cast of boosting, it's called a boost. But it can really be thought of as a step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13ad7d-90de-4977-b353-93d10a292445",
   "metadata": {},
   "source": [
    "# 1.2 Stepest-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff3629-30fa-4db8-9e0c-bafbfe1f6a8c",
   "metadata": {},
   "source": [
    "> <i>Steepest-descent is one of the simplest of the frequently used numerical minimization methods. It defines the increments $\\{p_m\\}_i^M$ as follows. FIrst the current gradient $g_m$ is computed\n",
    "<br />\n",
    "$g_m = \\{g_{jm}\\} = \\{ [\\frac{\\partial \\phi(P)}{\\partial P_j}]_{P = P_{m -1}}\\}$\n",
    "<br />\n",
    "where\n",
    "<br />\n",
    "$P_{m-1} = \\sum_{i = 0}^{m - 1}p_i$\n",
    "<br />\n",
    "The step is taken to be\n",
    "<br />\n",
    "$p_m = -\\rho_m g_m$\n",
    "<br />\n",
    "where\n",
    "<br />\n",
    "$\\rho_ = \\text{argmin}_{\\rho} \\Phi (P_{m -1} - \\rho g_m)$\n",
    "<br />\n",
    "The negative gradient $-g_m$ is said to define the \"steepest-descent\" direction and (5) is called the \"line search\" along that direction.</i>\n",
    "\n",
    "Rememeber, steepest-descent is a special case of gradient descent where the step size ($\\rho$) is taken such that the following function is at its lowest point:\n",
    "\n",
    "$\\Phi (P_{m -1} - \\rho g_m)$\n",
    "\n",
    "For a neural network, $P_{m -1}$ would the weight vector before the update is applied. \n",
    "\n",
    "We take the gradient of our loss function with respect to the parameter in context (the $jth$ parameter) for the _mth_ model.\n",
    "\n",
    "💻 Let's clear this up with some code. For examples sake, I'm going to use a brute force approach and try a bunch of values for the line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f02cefe-6b9e-4fc0-baad-7aac94ce95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1118c3220>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function we want to minimize\n",
    "def f(w):\n",
    "    return 5*(w + 2)**2\n",
    "\n",
    "# It's gradient\n",
    "def grad_f(w):\n",
    "    return 2 * (w - 5)\n",
    "\n",
    "w = np.arange(-7, 5, step=.1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w, f(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166df961-f4d7-4954-8a9c-ab90844c4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_steepest_descent_step(w, f, grad_f, step_size=1.,\n",
    "                             steepest_descent=False):\n",
    "    \n",
    "    # Evaluate current parameter at gradient\n",
    "    grad = grad_f(w)\n",
    "    print(f'Grad value: {grad}')\n",
    "    \n",
    "    # Brute force approach:\n",
    "    # try a vector of candidates for line search\n",
    "    if steepest_descent:\n",
    "        step_sizes = np.arange(0, 0.3, step=1e-5)\n",
    "        y = f(w - step_sizes * grad)\n",
    "        step_size = step_sizes[np.argmin(y)]\n",
    "        print(f'Best step size: {step_size}')\n",
    "    \n",
    "    # We add \"\"-step_size * grad\"\n",
    "    # to w. So gradient descent is really\n",
    "    # about adding a bunch of values\n",
    "    # until we are dones just like the paper\n",
    "    # points out\n",
    "    \n",
    "    w_new = w - step_size * grad\n",
    "        \n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a515d086-ab78-4b07-b481-733585c300f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "w_before=10.0\n",
      "Grad value: 10.0\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=7.0001\n",
      "\n",
      "Step 2\n",
      "w_before=7.0001\n",
      "Grad value: 4.0001999999999995\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.800080002\n",
      "\n",
      "Step 3\n",
      "w_before=5.800080002\n",
      "Grad value: 1.6001600039999992\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.32004800240004\n",
      "\n",
      "Step 4\n",
      "w_before=5.32004800240004\n",
      "Grad value: 0.6400960048000801\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.128025601920064\n",
      "\n",
      "Step 5\n",
      "w_before=5.128025601920064\n",
      "Grad value: 0.25605120384012814\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.051212801280064\n",
      "\n",
      "Step 6\n",
      "w_before=5.051212801280064\n",
      "Grad value: 0.10242560256012823\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.020486144768051\n",
      "\n",
      "Step 7\n",
      "w_before=5.020486144768051\n",
      "Grad value: 0.04097228953610177\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.008194867630116\n",
      "\n",
      "Step 8\n",
      "w_before=5.008194867630116\n",
      "Grad value: 0.016389735260231575\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.003278110949399\n",
      "\n",
      "Step 9\n",
      "w_before=5.003278110949399\n",
      "Grad value: 0.006556221898797787\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.001311309941978\n",
      "\n",
      "Step 10\n",
      "w_before=5.001311309941978\n",
      "Grad value: 0.0026226198839562898\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.00052455020299\n",
      "\n",
      "Final value: 5.00052455020299\n"
     ]
    }
   ],
   "source": [
    "w = 10.\n",
    "for i in range(10):\n",
    "    print(f'Step {i + 1}')\n",
    "    print(f'w_before={w}')\n",
    "    w_new = do_steepest_descent_step(w, f, grad_f, steepest_descent=True)\n",
    "    if w == w_new:\n",
    "        print('Convergence reached!')\n",
    "        break\n",
    "    w = w_new\n",
    "    print(f'w_after={w}')\n",
    "    print('')\n",
    "print(f'Final value: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a04ed-1c35-4870-ad99-25cd2ef49407",
   "metadata": {},
   "source": [
    "# 2 Numerical optimiizing in function space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b159a1-34ee-46b1-98fb-83b9047a5eb3",
   "metadata": {},
   "source": [
    "> <i>Here we take a \"nonparametric\" approach and apply numerical optimization in function space. That is we consider F(x) evaluated at each point x to be a \"parameter\" and seek to minimize\n",
    "<br />\n",
    "<br />\n",
    "$\\Phi(F) = E_{y,x}L(y, F(x)) = E_x[E_y [L(y, F(x) | x]]$\n",
    "<br />\n",
    "<br />\n",
    "or equivalently\n",
    "<br />\n",
    "<br />\n",
    "$\\Phi(F(x)) = E_y [L(y, F(x)) | x]$\n",
    "<br />\n",
    "<br />\n",
    "at each individual x, directly with respect to F(x). \n",
    "</i>\n",
    "\n",
    "If we treat each $F(x)$ as a parameter, there will be as many parameters as there are unique x values. In practice, you will only be given a finite number of parameter observations (the training data).\n",
    "\n",
    "This is similar to instance-based methods (you store the data and use at runtime) in ML. For example, in the nearest neighbor model, you store the data and when picking a point, find the 5 nearest neighbors for example. The data itself ARE the parameters of the model. But the parameters do not need to be estimated, they are just given. But this paper is not using an instance-based approach, we still want to estimate the parameter at each point. This is the only way we can make predictions for the parameters not observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36647c6d-57f9-4771-a932-9a5f87575a57",
   "metadata": {},
   "source": [
    "> <i>In function space there are an infinite number of such parameters, but in data sets (discussed below) only a finite number $\\{F(x_i)\\}_i^N$ are involved. Following the numerical optimization paradigm we take the solution to be\n",
    "<br />\n",
    "<br />\n",
    "$F^*(x) = \\sum_m f_m(x)$\n",
    "<br />\n",
    "<br />\n",
    "where $f_0(x)$ is an initial guess, and ${f_m(x)}_1^M$ are incremental functions (\"steps\" or \"boosts\") defined by the optimization method.\n",
    "</i>\n",
    "\n",
    "We only have as many parameters as training examples. If are treating the function as a parameter, taking the sum approach like how parameters are estimated makes sense. In the parameter approach, we end up with a final vector of parameters that is then used in our model (which has a fixed structure). Here, we actually end up with the model outputs instead.\n",
    "\n",
    "So this is basically an exercise in notation. Everwhere there is a $p$ just replace with $f_m$. But this raises a major practical issue: what happens when you want to get an output for an $x$ not in the training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1410c-2adb-4807-9ea8-81dd31e9a222",
   "metadata": {},
   "source": [
    "> <i>For steepest-descent\n",
    "<br />\n",
    "<br />\n",
    "$f_m(x) = -\\rho_m g_m(x)$\n",
    "<br />\n",
    "<br />\n",
    "with\n",
    "<br />\n",
    "<br />\n",
    "$g_m(x) = [\\frac{\\partial \\phi(F(x))}{\\partial F(x)}]_{F(x) = F_{m - 1}(x)} = [\\frac{\\partial E_y [L(y, F(x) | x]}{\\partial F(x)}]_{F(x) = F_{m - 1}(x)}$\n",
    "<br />\n",
    "<br />\n",
    "and\n",
    "<br />\n",
    "<br />\n",
    "$F_{m - 1}(x) = \\sum_{i=0}^{m-1} f_i(x)$\n",
    "<br />\n",
    "<br />\n",
    "Assuming sufficient regularity that one can interchange differentiation and integration, this becomes\n",
    "<br />\n",
    "<br />   \n",
    "$g_m(x) = E_y[\\frac{\\partial L(y, F(x))}{\\partial F(x)}]_{F(x) = F_{m-1}(x)}$\n",
    "<br />\n",
    "<br />  \n",
    "The multiplier $\\rho_m$ in (6) is given by the line search\n",
    "<br />\n",
    "<br /> \n",
    "$\\rho_m = \\text{argmin}_{\\rho_m} E_{y, x} L(y, F_{m - 1}(x) - \\rho_m g_m(x))$\n",
    "</i>\n",
    "\n",
    "Here, $\\rho_m$ is calculated with the line search (like the naive implementation we did earlier). In a package like XGBoost, it's known as the \"eta\" parameter. AFAIK, it's a hyperparameter this is constant across all \"boosts\".\n",
    "\n",
    "Recall that $E_y [L(y, F(x)) | x]$ is $\\int_y L(y, F(x)) P(y | x) dy$. For a given $x$ then, for the true underlying function, what is the probability of y given that x? This must be 1 when y is correct, and 0 otherwise. So this really just picks out $L(y, F(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc404a9-0870-451d-a12d-1e0f04e6f504",
   "metadata": {},
   "source": [
    "# 3 Finite data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247f6a1-8bf0-4671-991f-ef923fae0292",
   "metadata": {},
   "source": [
    "> <i>This nonparametric approach breaks down when the joint distribution of (y, x) is estimated by a finite data sample $\\{y_i, x_i\\}_i^N$. In this case $E_y[. | x]$ cannot be estimated accurately by its data value at each $x_i$, and even if it could, one would like to estimate $F^*(x)$ at x values other than the training sample points. Strength must be borrowed from nearby data points by imposing smoothness on the solution. One way to do this is to assume a parameterized form such as (2) and do parameter optimization as discussed in Section 1.1 to minimize the corresponding data based estimate of expected loss\n",
    "<br />\n",
    "<br />\n",
    "$\\{ \\beta_m, a_m\\}_i^M = \\text{argmin}_{\\beta_m, \\alpha_m} \\sum_{i=1}^N L \\big( y_i, \\sum_m \\beta_m h(x_i; a_m) \\big)$\n",
    "</i>\n",
    "\n",
    "This means we are back to assuming a parameterized model! And the model assumed here, is an additive one. Also, this optimization is done jointly--which gets harder and harder to do as the number of terms grows and the model complexity grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525c29f-2717-453d-a178-a0d902101115",
   "metadata": {},
   "source": [
    "> <i>In situations where this is infeasible one can try a \"greedy-stagewise\" approach. For $m = 1, 2, ..., M$\n",
    "<br />\n",
    "$(\\beta_m, \\alpha_m) = argmin_{\\beta, a} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\beta h(x_i; a)$\n",
    "<br />\n",
    "and then\n",
    "<br />\n",
    "$F_m(x) = F_{m - 1}(x) + \\beta_m h(x; a_m)$</i>\n",
    "\n",
    "So the tradeoff here is that we won't in general, obtain the correct solution. By freezing the set of parameters at each 'stage', this means we only have to learn the the weight and parameters of the given booster one at a time. It does make me wonder how much of a tradeoff this is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bb24e-0fed-4cb9-bdc1-c9721424e5f6",
   "metadata": {},
   "source": [
    "> <i>Note that this stagewise strategy is different from stepwise approaches that readjust previously entered terms when new ones are added. In signal processing this stagewise strategy is called \"matching pursuit\" where L(y, F) is squared-error loss and the $\\{h(x; a_m\\}_1^M$ are called basis functions, usually taken from an over-complete wavelet-like dictionary. In machine learning, (9) (10) is called \"boosting\" where $y \\in \\{-1, 1\\}$ and $L(y, F)$ is either an exponential loss criterion $e^{-yF}$ or negative binomial log-likelihood. The function h(x; a) is called a \"weak learner\" or \"base learner\", and is usually a classification tree.</i>\n",
    "\n",
    "The \"matching pursuit\" algorithms arises from the paper <a href=\"https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf\">Matching Pursuits with Time-Frequency Dictionaries</a>. Uforunately, not much more I can say about it at this time.\n",
    "\n",
    "I have not studied much on the topic of sparse coding. But here's what I could gather. A vector is represented by a linear combination of a set of basis vectors (in general, no requirement these be orthogonal). When we say basis, what we mean is that each vector is UNIQUELY represented by a set of components (the coefficients in the linear sum of basis vectors). But it seems while mathematically nice, it's not always practical. It turns having the ability to represent a vector in more than one way brings benefits (I cannot articulate why at this time). This means we will need to more atoms than we need. This is where the term \"over-complete dictionary\" comes in. The wavelet refers to the type of $h(x; a)$ model we assume. The exponential loss is referring back to the Adaboost paper. Also, $h(x; a)$ is also called a \"booster\" as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe3508-4bc5-4bcb-b443-f1088e6305a6",
   "metadata": {},
   "source": [
    "><i>Suppose for that for a particular loss $L(y, F)$ and/or base learner $h(x; a)$ the solution to $(\\beta_m, \\alpha_m) = argmin_{\\beta, a} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\beta h(x_i; a)$ is difficult to obtain. Given any approximator $F_{m-1}(x)$, the function $\\beta_m h(x; a_m)$ can be viewed the best greedy step towards the data based estimate of $F^*(x)$, under the constraint that the step \"direction\" $h(x; a_m)$ be a member of the parameterized class of functions $h(x;a)$. It can thus be regarded as a steepest-descent step under that constraint. By construction, the data based analogue of the unconstrained negative gradient\n",
    "<br/>\n",
    "<br/>\n",
    "$-g_m(x_i) = - \\big[ \\frac{\\partial L(y_i, F(x_i)}{\\partial F(x_i)} \\big]_{F(x) = F_{m - 1}(x)}$\n",
    "<br/>\n",
    "<br/>\n",
    "gives the best steepest-descent step direction $-g_m = \\{ -g_m(x_i)\\}_1^N$ in the N-dimensional data space at $F_{m-1}(x)$\n",
    "</i>\n",
    "\n",
    "What this is saying, is that we create a vector of made up N components, where a single component is $L(y_i, \\hat{y_i})$. By taking the gradient of this vector, and taking a step in the opposite direction of this gradient, we are approaching a point in this space where the loss becomes a minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e383c9-fd2c-4db5-a8a8-223142e35ed8",
   "metadata": {},
   "source": [
    "> <i>However, this gradient is only defined only at the data points $\\{ x_i\\}_i^N$ and cannot be generalized to other x-values. One possibility for generalization is to choose that member of the parameterized class $h(x; a_m)$ that produces $h_m = \\{ h(x_i; a_m \\}_{1}^N$ most parallel to $-g_m \\in R^N$. This is the $h(x; a)$ most highly correlated with $-g_m(x)$ over the data distribution. It can be obtained from the solution:\n",
    "<br />\n",
    "<br />\n",
    "$a_m = \\text{argmin}_{a, \\beta} \\sum_i^N [-g_m(x) - \\beta h(x_i; a)]^2$\n",
    "</i>\n",
    "\n",
    "Realize, we are back to a parameterized model. But what's novel here, is that are trying to learn the gradient of the loss function. The $h(x_i; a)$ is now a model that is trying to replicate the loss gradient. So this is a different type of learning. We can call this \"gradient learning\".\n",
    "\n",
    "Now, because of this, a new and different cost function is introduced. Here, the author chooses least squares (but of course other cost functions can be used). By learning the gradient, this allows the model to be defined for $x$ not in the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a426ffc-5bf8-4930-8c0c-1d9a52fa19be",
   "metadata": {},
   "source": [
    "> <i>This constrained negative gradient $h(x; a_m)$ is used in place of the unconstrained one in the steepest-descent strategy. Specifically, the line search is performed\n",
    "<br/>\n",
    "<br/>\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, F_{m - 1}(x_i) - \\rho h(x_i; a_m))$\n",
    "<br/>\n",
    "<br/>\n",
    "and the approximation updated\n",
    "<br/>\n",
    "<br/>\n",
    "$F_m(x) = F_{m-1}(x) + \\rho_m h(x; a_m)$\n",
    "<br/>\n",
    "<br/>\n",
    "Basically, instead of obtaining the solution under a smoothness constraint, the constraint is applied to the unconstrained (rough) solution by fitting $h(x; a)$ to the \"pseudo-responses\" $\\{ \\tilde{y_i} = -g_m(x_i)\\}_{i=1}^N$. This permits the replacement for the difficult function minimization problem by least-squares function minimization, followed by only a single parameter optimization based on the original criterion. Thus, for any $h(x; a)$ for this a feasible least-squares algorithm exists for solving $a_m = \\text{argmin}_{a, \\beta} \\sum_i^N [-g_m(x) - \\beta h(x_i; a)]^2$, one can use this approach to minimize any differential loss $L(y, F)$ in conjunction with forward stagewise additive modeling.\n",
    "    \n",
    "The uncontrained so-called \"rough\" solution is using the training gradient values (the value from calculating the deriative of the loss function). These values are what Friedman calls the \"pseudo-responses\". I mean, it's not the response value itself (which is $y$). But it's obtained after feeding our $y$ into the loss function and taking the first derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed57fd7-8046-41aa-ae04-49cb29186322",
   "metadata": {},
   "source": [
    "><i>This leads to the following (generic) algorithm using steepest-descent.\n",
    "<br />\n",
    "$F_0(x) = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, \\rho)$\n",
    "<br />\n",
    "For $m=1$ to M do:\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $y_i = - \\big[ \\frac{\\partial L(y_i, F(x_i)}{\\partial F(x_i)} \\big]_{F(x) = F_{m-1}(x)}$, over $i=1, N$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $a_m = \\text{argmin}_{a, \\beta} \\sum_i^N [\\tilde{y_i} - \\beta h(x_i; a)]^2$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\rho h(x_i; a_m))$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $F_m(x) = F_{m - 1}(x) + \\rho_m h(x; a_m)$\n",
    "<br />\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8978fd1-7dac-4390-aa06-dbef7d3239f2",
   "metadata": {},
   "source": [
    "💻 Let's implement this now. We will calculate the gradients manually. But to support general loss functions, we would need to rely on a package like <a href=\"https://jax.readthedocs.io/en/latest/index.html#\">jax</a>. We will gradually build up our class thoughout the paper to add all the loss functions covered.\n",
    "\n",
    "Our first step is to get some data to try our algorithm on. Let's leverage sklearn for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "446fb265-ea77-4bac-9148-91d49c3b939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a regression dataset\n",
    "# that requires several predictors to solve\n",
    "import sklearn\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "d7053b51-45e3-4fd0-9d18-92fea6d06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sklearn.datasets.fetch_california_housing(return_X_y=False, download_if_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "f059a31f-7480-490f-b642-69095534d689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Median house value in units of $100,000\n",
    "y = ds.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "f47438ce-6c54-464b-8e80-95a901db28b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will ignore lat and long for our purposes\n",
    "# MedInc     = median income in block group\n",
    "# HouseAge   = median house age in block group\n",
    "# AveRooms   = average number of rooms per household\n",
    "# AveBedrms  = average number of bedrooms per household\n",
    "# Population = block group population\n",
    "# AveOccup   = average number of household members\n",
    "# Latitude   = block group latitude\n",
    "# Longitude  = block group longitude\n",
    "X = ds.data[:,0:-2]\n",
    "ds.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "cf2b1ea6-368a-4412-a4b4-33f62f179622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train=16512\n",
      "n_test=4128\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y,\n",
    "                                                                            test_size=.2,\n",
    "                                                                            random_state=random_state)\n",
    "\n",
    "X_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "y_scaler = preprocessing.StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train = y_scaler.transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f'n_train={X_train.shape[0]}')\n",
    "print(f'n_test={X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "479f2635-16f9-43ae-b289-e998a16eb2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.0000000000000002\n",
      "Test MSE: 1.0219064789978365\n"
     ]
    }
   ],
   "source": [
    "# For a baseline, let's use an average as the model\n",
    "train_error = mean_squared_error(y_train, [np.mean(y_train)] * len(y_train))\n",
    "test_error = mean_squared_error(y_test, [np.mean(y_train)] * len(y_test))\n",
    "print(f'Train MSE: {train_error}')\n",
    "print(f'Test MSE: {test_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "57bc831b-b623-4433-a56f-2debf0e2eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.5562942729397101\n",
      "Test MSE: 0.5590669729422854\n"
     ]
    }
   ],
   "source": [
    "# Compare to a single decision tree\n",
    "# We can of course, play with different max depths\n",
    "# Not setting max depth will immediately cause\n",
    "# the model to overfit. Try it for yourself.\n",
    "decision_tree_baseline = DecisionTreeRegressor(max_depth=2)\n",
    "decision_tree_baseline.fit(X_train, y_train)\n",
    "train_error = mean_squared_error(y_train, decision_tree_baseline.predict(X_train))\n",
    "test_error = mean_squared_error(y_test, decision_tree_baseline.predict(X_test))\n",
    "print(f'Train MSE: {train_error}')\n",
    "print(f'Test MSE: {test_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745fff3-b897-45a6-9a65-87408db59f8f",
   "metadata": {},
   "source": [
    "It's a little bit better. Let's try linear regression next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "0d03415e-59a5-4ecb-86ef-81e3842d9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.46247902995661444\n",
      "Test MSE: 0.4700960671835673\n"
     ]
    }
   ],
   "source": [
    "# Let's see what standard linear gression gives us\n",
    "model_baseline = LinearRegression()\n",
    "model_baseline.fit(X_train, y_train)\n",
    "train_error = mean_squared_error(y_train, model_baseline.predict(X_train))\n",
    "test_error = mean_squared_error(y_test, model_baseline.predict(X_test))\n",
    "print(f'Train MSE: {train_error}')\n",
    "print(f'Test MSE: {test_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e152184-e4df-4739-9d33-c70f03a818ae",
   "metadata": {},
   "source": [
    "Now we implement GBMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "30eadc35-bb47-4398-aeaf-650869e4cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBM():\n",
    "    def __init__(self,\n",
    "                 loss_fn,\n",
    "                 booster,\n",
    "                 n_boost,\n",
    "                 step_size=0.001,\n",
    "                 X_test=None,\n",
    "                 y_test=None):\n",
    "        \n",
    "        self.n_boost = n_boost\n",
    "        self.step_size = step_size\n",
    "        self.booster = booster\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    " \n",
    "        if loss_fn == 'least_squares':\n",
    "            self.loss_fn = lambda y, y_hat: (y - y_hat)**2 / 2.\n",
    "            self.grad_loss_fn = lambda y, y_hat: -(y - y_hat)\n",
    "        else:\n",
    "            raise ValueError(f'{loss_fn} is an unsupported loss function')\n",
    "        # Set during fit\n",
    "        # But set here for visibility sake\n",
    "        self.prior = None\n",
    "        self.boosters = []\n",
    "        self.training_loss = []\n",
    "        self.test_loss = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        self.prior = np.mean(y)\n",
    "\n",
    "        y_hat = np.array([self.prior] * len(y))\n",
    "        \n",
    "        self.training_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        for _ in range(self.n_boost):\n",
    "            \n",
    "            # Clone a model\n",
    "            model = clone(self.booster)\n",
    "            \n",
    "            # Calculate negative gradient values\n",
    "            y_train = self.grad_loss_fn(y, y_hat)\n",
    "                        \n",
    "            # Build a model that learns\n",
    "            # the negative of the gradient\n",
    "            model.fit(X, y_train)\n",
    "            \n",
    "            # Predict gradient values\n",
    "            grad_hat = model.predict(X)\n",
    "            \n",
    "            # Take a step so that loss function\n",
    "            # decreases\n",
    "            y_hat = y_hat - self.step_size * grad_hat\n",
    "            \n",
    "            # Save model for future inferences\n",
    "            self.boosters.append(model)\n",
    "            \n",
    "            # Predict what we have thus far\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # Compute loss at each step\n",
    "            train_loss = np.mean(2 * self.loss_fn(y, y_pred))\n",
    "            \n",
    "            self.training_loss.append(train_loss)\n",
    "            \n",
    "            if self.X_test is not None:\n",
    "                y_pred = self.predict(self.X_test)\n",
    "                test_loss = np.mean(2 * self.loss_fn(self.y_test, y_pred))\n",
    "                self.test_loss.append(test_loss)\n",
    "                \n",
    "        self.training_time = time.time() - t0\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_hat = np.array([self.prior] * X.shape[0])\n",
    "        \n",
    "        for booster in self.boosters:\n",
    "            \n",
    "            # Make a prediction\n",
    "            grad_hat = booster.predict(X)\n",
    "            \n",
    "            # Take step\n",
    "            y_hat = y_hat - self.step_size * grad_hat\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "474f30b2-2c13-49cb-a425-431521450713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:    0.35506350867456676\n",
      "Test MSE:     0.35872341259179746\n",
      "Traing Time:  1.8041279315948486 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13587d9a0>"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = GBM(loss_fn='least_squares',\n",
    "            booster=DecisionTreeRegressor(max_depth=2, criterion='friedman_mse'),\n",
    "            n_boost=50,\n",
    "            step_size=.1,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_error = mean_squared_error(y_train, model.predict(X_train))\n",
    "test_error = mean_squared_error(y_test, model.predict(X_test))\n",
    "\n",
    "print(f'Train MSE:    {train_error}')\n",
    "print(f'Test MSE:     {test_error}')\n",
    "print(f'Traing Time:  {model.training_time} seconds')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(model.training_loss)), model.training_loss, marker='o', color='gray', label='train')\n",
    "ax.plot(range(len(model.test_loss)), model.test_loss, marker='o', color='red', label='test')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_xlabel('Number of Rounds')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6602a-8d1e-4c6f-b42b-73e18a5bbc0b",
   "metadata": {},
   "source": [
    "> <i>Note that any fitting criterion that estimates conditional expecation (given x) could in principle be used to estimate the (smoothed) negative gradient at line 4 of Algorithm 1. Least-squares (11) is a natural choice owing to the superior computational properties of many least-squares algorithm\n",
    "\n",
    "Yes. It's really just learning the gradient of the loss. So any cost function will work. Least-squares is a reasonable default IMHO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f791e20-5313-451c-a51e-8b115c8cdb5a",
   "metadata": {},
   "source": [
    "><i> In the special case where $y \\in \\{-1, 1\\}$ and the loss function $L(y, F)$ depends on y and F only through their product $L(y, F) = L(yF)$, the analogy of boosting (9), (10) to steepest-descent minimization has been noted in the machine learning literature. Duffy and Hemlbold elegantly exploit this analogy to motivate their GeoLev and and GeoArc procedures. The quantity $yF$ is call the \"margin\" and the steepest-descent is performed in the space of margin values, rather than the space of function values F. The latter approach permits application to more general loss functions where the notion of margins is not apparent. Durker employes a different strategy of casting regression in the framework of classification in the context of the AdaBoost algorithm.\n",
    "    \n",
    "It seems they make aware they are not the first to generalize AdaBoost. AdaBoost was generalized to margin values but a generalization to optimizing over function space direclty is indeed, the most general. It's a nice framework, because it leads to new classes of algorithms.\n",
    "    \n",
    "I never read the paper behind GeoLev and GeoArc but it can be found here: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.4750&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5cb9a-a357-49ea-b04d-ef6146fca156",
   "metadata": {},
   "source": [
    "# 4. Applications: additive modeling\n",
    "\n",
    "> <i>In this section the gradient boosting strategy is applied to several popular loss critiera: least-squares (LS), least absolute devation (LAD), Huber (M), and logistic binomial log-likelihood (L). The first serves as a \"reality check\", whereas the others lead to new boosting algorithms.\n",
    "    \n",
    "These are all the cost functions we coded up from the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697c9ca-42f3-4f0e-a227-19773f991a2a",
   "metadata": {},
   "source": [
    "## 4.1 Least-square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef771583-abcc-4b56-b12d-1018d24635f9",
   "metadata": {},
   "source": [
    "> <i>4.1 Least-squares regression. Here $L(y, F) = \\frac{1}{2}(y - F)^2$. The pseudoresponse in line 3 of algorithm is $\\tilde{y_i} = y_i - F_{m - 1}(x_i)$\n",
    "    \n",
    "Yes, $-[\\frac{\\partial L(y, F)}{\\partial F}] = y - F$ (use chain rule to solve). So we fitting a model where our target is the residual. If the residual is positive, we are underestimating the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc0a3f-d004-48a7-9ac7-de4592c61539",
   "metadata": {},
   "source": [
    "><i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1ea89-95ae-4d5e-aa89-02648c6f6b2d",
   "metadata": {},
   "source": [
    "> <i>Thus, line 4 simply fits the current residuals and the line search (line 5) produces the result $\\rho_m = \\beta_m$, where $\\beta_m$ is the minimizing $\\beta$ of line 4. Therefore, gradient boosting on squared-error loss produces the usual stagewise approach of iteratively fitting the current residuals.\n",
    "<br/>\n",
    "<br/>\n",
    "ALGORITHM 2 (LS_Boost)\n",
    "<br/>\n",
    "$F_0(x) = \\bar{y}$ [take average]\n",
    "<br/>\n",
    "For $m=1$ to M do:\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $\\tilde{y_i} = y_i - F_{m-1}(x_i)$, $i=1, N$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $(a_m, \\rho_m) = \\text{argmin}_{a, \\rho} \\sum_i^N [\\tilde{y_i} - \\rho h(x_i; a)]^2$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $F_m(x) = F_{m - 1}(x) + \\rho_m h(x; a_m)$\n",
    "<br />\n",
    "</i>\n",
    "\n",
    "So line (5) referenced is this:\n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\rho h(x_i; a_m))$\n",
    "\n",
    "Since the loss is least-squares we get:\n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N (y_i - (F_{m-1}(x_i) + \\rho h(x_i; a_m)))^2$\n",
    "\n",
    "rearranging:\n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N (y_i - (F_{m-1}(x_i)) - \\rho h(x_i; a_m)))^2$\n",
    "\n",
    "and since $y_i - F_{m-1}(x_i) = \\tilde{y_i}$ \n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N [\\tilde{y_i} - \\rho h(x_i; a_m)]^2$\n",
    "\n",
    "That's why the same equation gets repeated twice. Once for $\\rho_m$ and another time for $\\alpha_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcb3b9-7220-4be2-98b1-949467469ca4",
   "metadata": {},
   "source": [
    "## 4.2 Least absolute deviation (LAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815d691-4576-4c0d-80bc-64508ef89644",
   "metadata": {},
   "source": [
    "><i> For the loss function L(y, F) = |y - F|, one has\n",
    ">\n",
    ">$\\tilde{y_i} = -[\\frac{\\partial L(y_i, F(x_i)}{\\partial F(x_i)}]_{F(x) = F_{m-1}(x)} = \\text{sign}(y_i - F_{m-1}(x_i))$ \n",
    "\n",
    "This can be shown as follows. We can break the loss into a conditional statement:\n",
    "    \n",
    "$L(y_i, F(x_i)) = y_i - F(x_i)$ [for $y_i > F(x_i)$]\n",
    "    \n",
    "$L(y_i, F(x_i)) = F(x_i) - y_i$ [for $F(x_i) > y_i$]\n",
    "    \n",
    "$L(y_i, F(x_i)) = 0$ [for $F(x_i) = y_i$]\n",
    "    \n",
    "Taking the partial deriative piecewise we get:\n",
    "    \n",
    "$-1$ [for $y_i > F(x_i)$]\n",
    "    \n",
    "$+1$ [for $F(x_i) > y_i$]\n",
    "    \n",
    "$\\text{undefined}$ [for $F(x_i) = y_i$]\n",
    "    \n",
    "This is equivalent to $- \\text{sign}(y_i - F(x_i))$. (Notice the minus in front!). Remember, they are learning the negative of the gradient. So when the step is taken, you add the step (not subtract). Also, notice we are learning the sign of the error. And that even if the error is large, its magnitude is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9ac2d-5699-4e81-989f-3c15790592b5",
   "metadata": {},
   "source": [
    "> <i>This implies that $h$ is fit (least-squares) to the sign of the current residuals in > line 4 of Algorithm 1. The line search (line 5) becomes\n",
    ">\n",
    ">$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N |y_i - F_{m-1}(x_i) - \\rho h(x_i; a_m)|$\n",
    ">$= \\text{argmin}_{\\rho} \\sum_i^N |h(x_i; a_m)| |\\frac{y_i - F_{m-1}(x_i)}{h(x_i; a_m)} - \\rho|$\n",
    "><br />\n",
    ">$= \\text{median}_w \\{\\frac{y_i - F_{m-1}(x_i)}{h(x_i; a_m)}\\}$ &nbsp;&nbsp;&nbsp; $w_i = |h(x_i; a_m)|$\n",
    "    \n",
    "The key to understanding the solution to this, is to see this <a href=\"https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-ell-1-norm\">answer</a>. That is, the solution to the following:\n",
    "    \n",
    "$\\sum_i^N |x_i - \\alpha|$\n",
    "\n",
    "If $\\alpha$ is the median, then we are able to minimize the quantity. But for our case, is that each number in the series is weighted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891ca3f-cffa-4648-a44a-e0a769964076",
   "metadata": {},
   "source": [
    "><i> Here median_W{.} is the weighted median with weights $w_i$. Inserting these results into Algorithm 1 yields an algorithm for least absolute deviation boosting, using any base learner $h(x|a)$\n",
    "    \n",
    "Nice thing about this, is that the optimal learning rate is able to be computed at each step.\n",
    "    \n",
    "💻 Let's implement this now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42138d99-282d-4088-8453-6bac7212df90",
   "metadata": {},
   "source": [
    "> Here we consider the special case where each base learner is an J-terminal node regression tree. Each regression tree model itself the has additive form:\n",
    ">\n",
    "> $h(x; \\{b_j, R_j\\}_1^J) = \\sum_j^J b_j1 (x \\in R_j)$\n",
    "\n",
    "This means the function maps an example to a leaf node. Each leaf is a partition of feature space where these partitions do not overlap. The $b_j$ is the value you get at the $jth$ leaf. The $1 (x \\in R_j)$ is just an indicator function. So if $x$ falls in the $R_j$ region it becomes $1$, otherwise its 0. So to summarize:\n",
    "\n",
    "If $x$ in $R_j$ then return $b_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ff599-97b9-47e0-90e9-5d7f56ed707f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
