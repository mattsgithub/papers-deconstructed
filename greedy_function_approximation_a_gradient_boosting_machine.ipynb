{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21d290f-8915-4268-9f9b-0fdf28aa1fed",
   "metadata": {},
   "source": [
    "# PAPERS DECONSTRUCTED\n",
    "## Title: Greedy Function Approximation: A Gradient Boosting Machine\n",
    "### Authors: Jerome H. Friedman\n",
    "Link: https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172d037-6bec-47b9-bcba-d0c744e7089f",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "Most machine learning algorithms by finding the best parameter values for a loss function and training dataset. Gradient descent is typically used to solve this. This works (roughly) as follows:\n",
    "\n",
    "(1) Fetch the true value for the given training example\n",
    "\n",
    "(2) Compute a predicted value for the given training example\n",
    "\n",
    "(3) Compute the loss given the true value and predicted value\n",
    "\n",
    "(4) Compute the gradient of the loss with respect to the parameters\n",
    "\n",
    "In this paper step (2) is left as an unknown and the gradient is _directly_ learned from data instead. This is done by setting up a supervised machine learing problem of the following:\n",
    "\n",
    "Training Dataset = $\\{ (\\tilde{y_i}, x_i) \\}_i^N$ \n",
    "\n",
    "where $\\tilde{y_i}$ is the gradient of the loss with respect to the prediction value evaluated at the _previous_ prediction value. Learning the gradient is repeated at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79267e60-984d-458b-84fe-a399ffe0b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "import sklearn\n",
    "from sklearn.datasets import make_regression\n",
    "from scipy.special import expit\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "random_state=22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ab196-ebea-4389-85f8-2e58ea04cf1e",
   "metadata": {},
   "source": [
    "# Full Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebfdb6-ead1-4e5f-a27e-d26c7d41f790",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16513821-f8d9-40df-a23f-038cb12b2dd0",
   "metadata": {},
   "source": [
    "> <i>Function estimation/approximation is viewed from the perspective of\n",
    "numerical optimization in function space, rather than parameter space</i>\n",
    "\n",
    "Instead of optimizing for a parameter value (like the slope value in linear regression), we are optimizing for a function directly. This is unusual. ML algorithms are typically designed to optimize over parameter values, not functions directly. We typically calculate something like $\\frac{\\partial J}{\\partial \\theta}$ (where J is the cost function and $\\theta$ is a parameter value) but here we are calculating something like $\\frac{\\partial J}{\\partial f(x)}$. We take a derivative with respect to a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b952c-4692-488e-9c0d-3456f40c5b4b",
   "metadata": {},
   "source": [
    "> <i>A connection is made between stagewise additive expansions and steepest descent minimization.</i>\n",
    "\n",
    "_steepest-descent minimization_ is not the same as _gradient descent_. See this <a href=\"https://stats.stackexchange.com/a/322177/30545\">link</a>. The link also includes a <a href=\"http://www.math.usm.edu/math/lambers/mat419/lecture10.pdf\">reference</a> that's helpful. Steepest-descent minimization is a special case of gradient descent. Gradient descent is using knowledge of the gradient to choose where to step next. But _how_ you take a step, leads you to specific algorithms (such as steepest-descent).\n",
    "\n",
    "In steepest-descent, the goal is to find a $\\alpha$ that minimizes $g$:\n",
    "\n",
    "$g({\\alpha}) = f(x_{t-1} - \\alpha \\nabla f(x_{t-1})) \\;\\;\\; \\alpha \\ge 0$\n",
    "\n",
    "I'm not a numerical specialist, but there's a tradeoff here. While we are exact (as opposed to using a fixed step size), this requires solving an additional optimization problem at each step. I would think the tradeoffs are problem specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb7008-b913-4ebb-83bb-372fc0607a89",
   "metadata": {},
   "source": [
    "> <i>A general gradient descent “boosting” paradigm is\n",
    "developed for additive expansions based on any fitting criterion.</i>\n",
    "\n",
    "Additive models are models where can break the model into a sum of models:\n",
    "\n",
    "f(x) = $\\sum_i f_i(x)$\n",
    "\n",
    "Stagewise (as opposed to _stepwise_) means we can't go back and edit. Each 'stage' freezes all previous terms. Section **3.3.3 Forward-Stagewise Regression** <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">The Elements of Statistical Learning</a> is helpful.\n",
    "\n",
    "The boosting paradigm mentioned is a reference to Freund and Schapire's <a href=\"https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf\">paper</a> in the late 90s that introduced the AdaBoost algorithm. The idea of boosting is to build a many models in a sequence. Each model learns from the mistakes from the previous model.\n",
    "\n",
    "By \"any fitting criterion\" means that this works (essentially) for any loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab1dfc-2da5-4fd2-89ea-67fa7d3d63de",
   "metadata": {},
   "source": [
    "> <i>Specific algorithms are presented for least-squares, least absolute deviation, and\n",
    "Huber-M loss functions for regression, and multiclass logistic likelihood\n",
    "for classification</i>\n",
    "\n",
    "Let's show what each of these losses look like mathematically and implemented in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ebcdcc4-a0ef-4a90-9b6c-66cd88f491dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we setup some data to demonstrate each function with\n",
    "\n",
    "# True function\n",
    "f_star = lambda x: x**2\n",
    "\n",
    "# Estimated function\n",
    "# (we introduce a little bit of error)\n",
    "f = lambda x: (x + 0.1)**2\n",
    "\n",
    "# Points to evaluate over\n",
    "x = np.arange(0, .5, step=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae5fd2-a28f-4760-ae93-4b73ca1ceeb4",
   "metadata": {},
   "source": [
    "**least-squares loss**\n",
    "\n",
    "$l(y, f) = \\frac{1}{2} (y - f)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28181974-64b4-4e2e-b2c6-0a8a038c277e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00e-05, 2.00e-04, 4.50e-04, 8.00e-04, 1.25e-03, 1.80e-03,\n",
       "       2.45e-03, 3.20e-03, 4.05e-03, 5.00e-03])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def least_squares(y_true, y_pred):\n",
    "    loss = .5 * (y_true - y_pred)**2\n",
    "    return loss\n",
    "\n",
    "least_squares(f_star(x), f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef921bf-ace0-44e5-9d58-bc1bb07fa3fc",
   "metadata": {},
   "source": [
    "**least-absolute deviation**\n",
    "\n",
    "$l(y, f) = |y - f|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9664703-59b6-44ca-b8db-90e043128314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def least_absolute_deviation(y_true, y_pred):\n",
    "    loss = np.abs(y_true - y_pred)\n",
    "    return loss\n",
    "\n",
    "least_absolute_deviation(f_star(x), f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f06a428-c0f7-4959-9804-8b96e43bfd23",
   "metadata": {},
   "source": [
    "**huber-m**\n",
    "\n",
    "if $|y - f| \\le \\delta$ then:\n",
    "\n",
    "$l(y, f) =  \\frac{1}{2} (y - f)^2$\n",
    "\n",
    "else:\n",
    "\n",
    "$l(y, f) =  \\delta(|y - f| - \\frac{\\delta}{2})$\n",
    "\n",
    "This is the first time I had across Huber loss. The paper can be found <a href=\"https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full\">here</a>.\n",
    "\n",
    "The \"m\" I believe comes from the term \"m-estimator\". I _think_ the m means \"maximum likelihood type\" estimator. See <a href=\"https://www.statisticalconsultants.co.nz/blog/m-estimators.html\">here</a>. It comes from robust statistical methods. Something I myself would need to learn more about. This function acts as a switch. When the error gets too high it acts as a switch to a loss that doesn't penalize as heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd21bf46-c616-4005-aff9-34604511aa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00e-05, 2.00e-04, 4.50e-04, 8.00e-04, 1.25e-03, 1.80e-03,\n",
       "       2.45e-03, 3.20e-03, 4.05e-03, 5.00e-03])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def huber(y_true, y_pred, delta=5.):\n",
    "    z = y_true - y_pred\n",
    "    z_abs = np.abs(z)\n",
    "    loss = np.where(z_abs <= delta, .5 * z**2, delta * (z_abs - delta/2.))\n",
    "    return loss\n",
    "\n",
    "huber(f_star(x), f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb4459-c304-4bd0-a34d-440332f95ea7",
   "metadata": {},
   "source": [
    "**multi-class logistic likelihood**\n",
    "\n",
    "$l(y, f) = - \\sum_k^{\\text{n_classes}} y_k \\text{log} \\, p_k$\n",
    "\n",
    "The variable k is to iterate over each class. $y_k$ is either 0/1 and $p_k$ is your models outputted \"probability\" for that class. I put the probability in quotes because you don't always get good probabiity estimates in practice.\n",
    "\n",
    "We can write down the special case of a binary label (0/1):\n",
    "\n",
    "$l(y, f) = -  \\big( y_k \\text{log} \\, p_k + (1 - y_k) \\text{log} \\, (1 - p_k) \\big)$\n",
    "\n",
    "This acts as a switch. When the label is 1 we get:\n",
    "\n",
    "$l(y, f) = - \\text{log} \\, p_k$\n",
    "\n",
    "When the label is 0 we get:\n",
    "\n",
    "$l(y, f) = - \\text{log} \\, (1 - p_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d4b82bb-9b56-4ae5-8b87-5843f929bfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68815968, 0.68196046, 0.67334717, 0.66238538, 0.64915934,\n",
       "       0.75627179, 0.61634377, 0.59701423, 0.57593942, 0.55329211])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logistic_likelihood(y_true, y_pred, delta):\n",
    "    loss = -(y_true * np.log(y_pred) + (1. - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "y_true = np.random.randint(0, 2, len(x))\n",
    "\n",
    "# Pass through sigmoid to get a number\n",
    "# between 0 and 1. Acts like a probability\n",
    "y_pred = sigmoid(f(x))\n",
    "\n",
    "logistic_likelihood(y_true, sigmoid(f(x)), delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ed08d-c1e2-47b9-9024-164ad28e1872",
   "metadata": {},
   "source": [
    "This is a function we use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee546ec-5dfc-470f-aea8-3448feacd31a",
   "metadata": {},
   "source": [
    "> <i>Special enhancements are derived for the particular case\n",
    "where the individual additive components are regression trees, and tools\n",
    "for interpreting such “TreeBoost” models are presented.</i>\n",
    "\n",
    "They exploit the special case when the additive model is comprised of decision trees--which they then call \"TreeBoost\". Note that <a href=\"https://xgboost.readthedocs.io/en/stable/\">XGBoost</a> by default uses TreeBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793c429-3d6d-490c-9de0-68a1376e4611",
   "metadata": {},
   "source": [
    "> <i>Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for\n",
    "mining less than clean data. Connections between this approach and the\n",
    "boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.</i>\n",
    "\n",
    "Lastly, they mention how all this ties in with the Adaboost algorithm (which introduced the idea of boosting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0524c49b-e99b-428b-b1d4-db7c1c5c03c0",
   "metadata": {},
   "source": [
    "# 1 Function Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ee195-4d0c-4839-a113-f4b1fb917367",
   "metadata": {},
   "source": [
    "> <i>In the function estimation or \"predictive learning\" problem, one has a system consisting of a random \"output\" or \"response\" variable y and a set of random \"input\" or \"explanatory\" variables $\\bf{x} = \\{x_1, ..., x_n\\}$</i>\n",
    "\n",
    "You are given labeled data (i.e., training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbc69fe-e621-4c48-a4f6-e1da3be7b5b5",
   "metadata": {},
   "source": [
    "><i>Using a “training” sample $\\{y_i, \\bf{x_i}\\}_{i}^{N}$\n",
    "1 of known $(y, \\bf{x})$-values, the goal is to\n",
    "obtain an estimate or approximation $\\hat{F}(\\bf{x})$, of the function $F^{*}(x)$ mapping $\\bf{x}$ to y, that minimizes the expected value of some specified loss function\n",
    "$L(y, F(\\bf{x}))$ over the joint distribution of all ($y, \\bf{x})$-values,\n",
    "<br/>\n",
    "<br/>\n",
    "$F^* = \\text{argmin}_F E_{y, \\bf{x}} L(y, F(\\bf{x})) = \\text{argmin}_F E_x [E_y(L(y, F(\\bf{x}))) | \\bf{x}]$\n",
    "</i>\n",
    "\n",
    "$F^{*}(x)$ is the true function. Unknown in practice (unless you are simulating data). But we can observe how inputs are relate to the outputs via labeled data. $F(x)$ is the function we learn from data. But to do learning, we need a measure of how good we are doing. Hence, the loss function:\n",
    "\n",
    "$L(y, F(x)) = L(F^{*}(x), F(x))$.\n",
    "\n",
    "A smaller loss value is better--less prediction error. We must compute the loss over each $(\\vec{x},y)$ pair. When the losses can be computed in parallel, we call that a univariate loss function. The opposite of this are multivariate losses. For example, see work done by Thorsten <a href=\"https://www.cs.cornell.edu/people/tj/publications/joachims_05a.pdf\">here</a>. Multivariate losses are used (for example) when designing a ranking algorithm.\n",
    "\n",
    "We want to summarize all these losses. A way to do this is to take a sum or an average. Typically the average loss is used.\n",
    "\n",
    "To get the form you see in this paper, I actually answered this many years ago <a href=\"https://math.stackexchange.com/a/623473/118474\">here</a>. But here it is again:\n",
    "\n",
    "$\\text{Expected Loss} = \\int_x \\int_y L(y, F(x)) P(x, y) \\, dx dy$\n",
    "\n",
    "By Bayes' theorem: $P(x, y) = P(x) P(y | x)$\n",
    "\n",
    "Let's substitute\n",
    "\n",
    "$\\int_x \\int_y L(y, F(x))  P(x) P(y | x) \\, dx dy$\n",
    "\n",
    "We can rearrange:\n",
    "\n",
    "$\\int_x P(x) \\big( \\int_y L(y, F(x)) P(y | x) \\, dy \\big) dx$\n",
    "\n",
    "And $\\int_y L(y, F(x)) P(y | x) dy$ is by definition $E_y [L(y, F(x) | x]$\n",
    "\n",
    "so \n",
    "\n",
    "$\\int_x P(x) \\big( E_y [L(y, F(x) | x] \\big) dx$\n",
    "\n",
    "But this is again, another expectation. So now we get:\n",
    "\n",
    "$E_x[E_y [L(y, F(x) | x]]$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$F^* = {\\text{argmin}}_F \\, E_{xy}[L(y, F(x))] = {\\text{argmin}}_F \\, \\int_x \\int_y P(x, y) L(y, F(x)) \\, dx dy$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9d1a0-628e-4cc5-8f37-fe2c69e26534",
   "metadata": {},
   "source": [
    "> <i>Frequently employed loss functions $L(y, F)$ include squared-error $(y − F^2)$ and absolute error $|y − F|$ for $y \\in R^1$ (regression) and negative binomial loglikelihood, $log(1 + e^{−2yF}$, when $y \\in \\{−1, 1\\}$ (classification).</i>\n",
    "\n",
    "This is telling us specific cases of what $L(y, F(X))$ can be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c0ab8-112b-4fe7-977f-0fd600d346fd",
   "metadata": {},
   "source": [
    "> <i>A common procedure is to restrict $F(\\bf{x})$ to be a member of a parameterized class of functions $F(\\bf{x}; \\bf{P})$, where $P = \\{P_1, P_2, ...\\}$ is a finite set of parameters whose joint values identify individual class members.</i>\n",
    "\n",
    "I will give a concrete example here. Take simple linear regression. Its takes the form of:\n",
    "\n",
    "$F(x) = b + mx$\n",
    "\n",
    "To rewrite in the notation just given:\n",
    "\n",
    "$F(x; \\bf{P} ) = p_1 + p_2 x$\n",
    "\n",
    "So $P = \\{p_1, p_2\\}$. Note that we fixed the function (basically) and now seek what the values of $b$ and $m$ are. I say basically because technically, each new set of parameters map out a different function, but the models _structure_ remains the same (i.e., linear form)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab351a-5815-4cda-ba13-9c3977be48e7",
   "metadata": {},
   "source": [
    "> <i>In this article we focus on \"additive\" expansions of the form $F(\\bf{x}; \\{\\beta_m, \\bf{a}_m\\}_1^M) = \\sum_m \\beta_m h(\\bf{x}; \\bf{a}_m)$</i>\n",
    "\n",
    "This additive expansion is saying, we have $m$ models that get added together. Each multiplied by a weight of $\\beta_m$. (The $m$ denotes which model we are referring to--there are $m$ of them). Each model is denoted as $h(x; a_m)$. The $a_m$ is telling us each model itself has a set of parameters that need to be estimated from data. And these models do not share parameters (would be interesting to think of use-cases where parameter sharing might be beneficial). Also, I see no reason why each model must keep the same structural form (decision tree vs naive bayes). But in practice, and even in this paper, $h(x; a_m)$ is left to be a decision tree. In the package XGBoost for example, $h(x; a_m)$ is referred to as the _booster_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f634b4-df4e-45eb-a775-b4d77fe9774d",
   "metadata": {},
   "source": [
    "> <i>The (generic) function $h(\\bf{x}, a)$ in (2) is usually a simple parameterized function of the input variables $\\bf{x}$, characterized by parameters $\\bf{a} = \\{a_1, a_2, ... \\}$. The individual terms differ in the joint values $a_m$ chosen for these parameters.</i>\n",
    "\n",
    "Yep. Each model will estimate $a_m$ to have different values than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c4ffa-f9ec-42ec-a25f-11b52ba049b1",
   "metadata": {},
   "source": [
    ">  <i>Such expansions (2) are at the heart of many function approximation methods such as neural networks, radial basis functions, wavelets, and support vector machines. Of special interest here is the case where each of the functions $h(\\bf{x}; \\bf{a}_m)$ is a small regression tree, such as those produced in CART. For a regression tree the parameters $\\bf{a}_m$ are the splitting variables, split locations, and the terminal node means of the individual trees.</i>\n",
    "\n",
    "They mention a bunch of ML models where this type of additive model is applicable. But the one of interest here, is the regression tree (i.e., decision tree). To parameterize this tree, at each node we have to choose a variable and a split point for that variable. So here they are saying that $a_m$ will denote the feature chosen for each node and its split point. We can count the number of parameters. At depth $l$ we will have $2^l$ nodes. So there are:\n",
    "\n",
    "$\\text{number of decision nodes} = \\sum_l^{D} 2^l$ where $D$ is the max depth\n",
    "\n",
    "$\\sum_l^{D} 2^l = 2^{D + 1} - 1$ (see <a href=\"https://math.stackexchange.com/a/1990146/118474\">proof</a>)\n",
    "\n",
    "And two parameters for each node (which variable and its split point):\n",
    "\n",
    "$ 2 * (2^{D + 1} - 1) = 2^{D + 2} - 2$\n",
    "\n",
    "I've built models in practice with depths reaching over 10 (even up to 15). So with $D = 15$ we get $131070$ parameters for a single decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04ed3dd-4f48-4c4f-90e6-cc1ebecd91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131070"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_params(max_depth):\n",
    "    n_param = (2**(max_depth + 2) - 2)\n",
    "    n_param_from_longer_calculation = 2 * sum([2**j for j in range(max_depth + 1)])\n",
    "    \n",
    "    # Evidence for proof\n",
    "    assert n_param == n_param_from_longer_calculation\n",
    "    \n",
    "    return n_param\n",
    "\n",
    "get_num_params(max_depth=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b16207-39a8-431b-a1ac-8570e39d604d",
   "metadata": {},
   "source": [
    "# 1.1 Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d354515-bb52-48cb-8f9b-9a7542f93245",
   "metadata": {},
   "source": [
    "> <i>In general, choosing a parameterized model $F(\\bf{x}; \\bf{P})$ changes the function optimization problem to one of parameter optimization\n",
    "<br />\n",
    "<br />\n",
    "$P^* = \\text{argmin}_P \\Phi(\\bf{P})$\n",
    "<br />\n",
    "<br />\n",
    "where\n",
    "<br />\n",
    "<br />\n",
    "$ \\Phi(P) = E_{y, x} L(y, F(x; P))$\n",
    "<br />\n",
    "<br />\n",
    "and then\n",
    "<br />\n",
    "<br />\n",
    "$ F^*(x) = F(x; P^*)$</i>\n",
    "\n",
    "Yes. In linear regression (for example) we use the loss function of least squares to find the best set of parameters. But the key here is that we fix the structure of the model and optimize over the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9c338-6e1d-4538-a647-d229e224612f",
   "metadata": {},
   "source": [
    "> <i>For most $F(x; P)$ and L, numerical optimization methods must be applied to solve $P^* = \\text{argmin}_P \\Phi(\\bf{P})$. This often involves expression the solution for the parameters in the form\n",
    "<br />\n",
    "<br />\n",
    "$P^* = \\sum_{m=0}^M p_m$\n",
    "<br />\n",
    "<br />\n",
    "where $p_0$ is the an initial guess and $\\{ p_m\\}_1^M $ are successive increments (\"steps\" or \"boosts\"), each based on the sequence of preceding steps. The precription for computing each step $p_m$ is defined by the optimization method.</i>\n",
    "\n",
    "in neural networks, we leverage gradient descent. The weight parameters get updated at each step by adding a value to them. Sp the final weights learned are a sum of the weight updates from each step. That is: \n",
    "\n",
    "$\\vec{w}_{final} = \\sum_i w_i$\n",
    "\n",
    "We start with an initial guess of parameters ($w_0$) and increment after each step. In the cast of boosting, it's called a boost. But it can really be thought of as a step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13ad7d-90de-4977-b353-93d10a292445",
   "metadata": {},
   "source": [
    "# 1.2 Stepest-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ff3629-30fa-4db8-9e0c-bafbfe1f6a8c",
   "metadata": {},
   "source": [
    "> <i>Steepest-descent is one of the simplest of the frequently used numerical minimization methods. It defines the increments $\\{p_m\\}_i^M$ as follows. FIrst the current gradient $g_m$ is computed\n",
    "<br />\n",
    "$g_m = \\{g_{jm}\\} = \\{ [\\frac{\\partial \\phi(P)}{\\partial P_j}]_{P = P_{m -1}}\\}$\n",
    "<br />\n",
    "where\n",
    "<br />\n",
    "$P_{m-1} = \\sum_{i = 0}^{m - 1}p_i$\n",
    "<br />\n",
    "The step is taken to be\n",
    "<br />\n",
    "$p_m = -\\rho_m g_m$\n",
    "<br />\n",
    "where\n",
    "<br />\n",
    "$\\rho_ = \\text{argmin}_{\\rho} \\Phi (P_{m -1} - \\rho g_m)$\n",
    "<br />\n",
    "The negative gradient $-g_m$ is said to define the \"steepest-descent\" direction and (5) is called the \"line search\" along that direction.</i>\n",
    "\n",
    "Rememeber, steepest-descent is a special case of gradient descent where the step size ($\\rho$) is taken such that the following function is at its lowest point:\n",
    "\n",
    "$\\Phi (P_{m -1} - \\rho g_m)$\n",
    "\n",
    "For a neural network, $P_{m -1}$ would the weight vector before the update is applied. \n",
    "\n",
    "We take the gradient of our loss function with respect to the parameter in context (the $jth$ parameter) for the _mth_ model.\n",
    "\n",
    "💻 Let's clear this up with some code. For examples sake, I'm going to use a brute force approach and try a bunch of values for the line search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f02cefe-6b9e-4fc0-baad-7aac94ce95e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11f6c79d0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The function we want to minimize\n",
    "def f(w):\n",
    "    return 5*(w + 2)**2\n",
    "\n",
    "# It's gradient\n",
    "def grad_f(w):\n",
    "    return 2 * (w - 5)\n",
    "\n",
    "w = np.arange(-7, 5, step=.1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(w, f(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166df961-f4d7-4954-8a9c-ab90844c4fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_steepest_descent_step(w, f, grad_f, step_size=1.,\n",
    "                             steepest_descent=False):\n",
    "    \n",
    "    # Evaluate current parameter at gradient\n",
    "    grad = grad_f(w)\n",
    "    print(f'Grad value: {grad}')\n",
    "    \n",
    "    # Brute force approach:\n",
    "    # try a vector of candidates for line search\n",
    "    if steepest_descent:\n",
    "        step_sizes = np.arange(0, 0.3, step=1e-5)\n",
    "        y = f(w - step_sizes * grad)\n",
    "        step_size = step_sizes[np.argmin(y)]\n",
    "        print(f'Best step size: {step_size}')\n",
    "    \n",
    "    # We add \"\"-step_size * grad\"\n",
    "    # to w. So gradient descent is really\n",
    "    # about adding a bunch of values\n",
    "    # until we are dones just like the paper\n",
    "    # points out\n",
    "    \n",
    "    w_new = w - step_size * grad\n",
    "        \n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a515d086-ab78-4b07-b481-733585c300f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "w_before=10.0\n",
      "Grad value: 10.0\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=7.0001\n",
      "\n",
      "Step 2\n",
      "w_before=7.0001\n",
      "Grad value: 4.0001999999999995\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.800080002\n",
      "\n",
      "Step 3\n",
      "w_before=5.800080002\n",
      "Grad value: 1.6001600039999992\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.32004800240004\n",
      "\n",
      "Step 4\n",
      "w_before=5.32004800240004\n",
      "Grad value: 0.6400960048000801\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.128025601920064\n",
      "\n",
      "Step 5\n",
      "w_before=5.128025601920064\n",
      "Grad value: 0.25605120384012814\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.051212801280064\n",
      "\n",
      "Step 6\n",
      "w_before=5.051212801280064\n",
      "Grad value: 0.10242560256012823\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.020486144768051\n",
      "\n",
      "Step 7\n",
      "w_before=5.020486144768051\n",
      "Grad value: 0.04097228953610177\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.008194867630116\n",
      "\n",
      "Step 8\n",
      "w_before=5.008194867630116\n",
      "Grad value: 0.016389735260231575\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.003278110949399\n",
      "\n",
      "Step 9\n",
      "w_before=5.003278110949399\n",
      "Grad value: 0.006556221898797787\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.001311309941978\n",
      "\n",
      "Step 10\n",
      "w_before=5.001311309941978\n",
      "Grad value: 0.0026226198839562898\n",
      "Best step size: 0.29999000000000003\n",
      "w_after=5.00052455020299\n",
      "\n",
      "Final value: 5.00052455020299\n"
     ]
    }
   ],
   "source": [
    "w = 10.\n",
    "for i in range(10):\n",
    "    print(f'Step {i + 1}')\n",
    "    print(f'w_before={w}')\n",
    "    w_new = do_steepest_descent_step(w, f, grad_f, steepest_descent=True)\n",
    "    if w == w_new:\n",
    "        print('Convergence reached!')\n",
    "        break\n",
    "    w = w_new\n",
    "    print(f'w_after={w}')\n",
    "    print('')\n",
    "print(f'Final value: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a04ed-1c35-4870-ad99-25cd2ef49407",
   "metadata": {},
   "source": [
    "# 2 Numerical optimiizing in function space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b159a1-34ee-46b1-98fb-83b9047a5eb3",
   "metadata": {},
   "source": [
    "> <i>Here we take a \"nonparametric\" approach and apply numerical optimization in function space. That is we consider F(x) evaluated at each point x to be a \"parameter\" and seek to minimize\n",
    "<br />\n",
    "<br />\n",
    "$\\Phi(F) = E_{y,x}L(y, F(x)) = E_x[E_y [L(y, F(x) | x]]$\n",
    "<br />\n",
    "<br />\n",
    "or equivalently\n",
    "<br />\n",
    "<br />\n",
    "$\\Phi(F(x)) = E_y [L(y, F(x)) | x]$\n",
    "<br />\n",
    "<br />\n",
    "at each individual x, directly with respect to F(x). \n",
    "</i>\n",
    "\n",
    "If we treat each $F(x)$ as a parameter, there will be as many parameters as there are unique x values. In practice, you will only be given a finite number of parameter observations (the training data).\n",
    "\n",
    "This is similar to instance-based methods (you store the data and use at runtime) in ML. For example, in the nearest neighbor model, you store the data and when picking a point, find the 5 nearest neighbors for example. The data itself ARE the parameters of the model. But the parameters do not need to be estimated, they are just given. But this paper is not using an instance-based approach, we still want to estimate the parameter at each point. This is the only way we can make predictions for the parameters not observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36647c6d-57f9-4771-a932-9a5f87575a57",
   "metadata": {},
   "source": [
    "> <i>In function space there are an infinite number of such parameters, but in data sets (discussed below) only a finite number $\\{F(x_i)\\}_i^N$ are involved. Following the numerical optimization paradigm we take the solution to be\n",
    "<br />\n",
    "<br />\n",
    "$F^*(x) = \\sum_m f_m(x)$\n",
    "<br />\n",
    "<br />\n",
    "where $f_0(x)$ is an initial guess, and ${f_m(x)}_1^M$ are incremental functions (\"steps\" or \"boosts\") defined by the optimization method.\n",
    "</i>\n",
    "\n",
    "We only have as many parameters as training examples. If are treating the function as a parameter, taking the sum approach like how parameters are estimated makes sense. In the parameter approach, we end up with a final vector of parameters that is then used in our model (which has a fixed structure). Here, we actually end up with the model outputs instead.\n",
    "\n",
    "So this is basically an exercise in notation. Everwhere there is a $p$ just replace with $f_m$. But this raises a major practical issue: what happens when you want to get an output for an $x$ not in the training dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1410c-2adb-4807-9ea8-81dd31e9a222",
   "metadata": {},
   "source": [
    "> <i>For steepest-descent\n",
    "<br />\n",
    "<br />\n",
    "$f_m(x) = -\\rho_m g_m(x)$\n",
    "<br />\n",
    "<br />\n",
    "with\n",
    "<br />\n",
    "<br />\n",
    "$g_m(x) = [\\frac{\\partial \\phi(F(x))}{\\partial F(x)}]_{F(x) = F_{m - 1}(x)} = [\\frac{\\partial E_y [L(y, F(x) | x]}{\\partial F(x)}]_{F(x) = F_{m - 1}(x)}$\n",
    "<br />\n",
    "<br />\n",
    "and\n",
    "<br />\n",
    "<br />\n",
    "$F_{m - 1}(x) = \\sum_{i=0}^{m-1} f_i(x)$\n",
    "<br />\n",
    "<br />\n",
    "Assuming sufficient regularity that one can interchange differentiation and integration, this becomes\n",
    "<br />\n",
    "<br />   \n",
    "$g_m(x) = E_y[\\frac{\\partial L(y, F(x))}{\\partial F(x)}]_{F(x) = F_{m-1}(x)}$\n",
    "<br />\n",
    "<br />  \n",
    "The multiplier $\\rho_m$ in (6) is given by the line search\n",
    "<br />\n",
    "<br /> \n",
    "$\\rho_m = \\text{argmin}_{\\rho_m} E_{y, x} L(y, F_{m - 1}(x) - \\rho_m g_m(x))$\n",
    "</i>\n",
    "\n",
    "Here, $\\rho_m$ is calculated with the line search (like the naive implementation we did earlier). In a package like XGBoost, it's known as the \"eta\" parameter. AFAIK, it's a hyperparameter this is constant across all \"boosts\".\n",
    "\n",
    "Recall that $E_y [L(y, F(x)) | x]$ is $\\int_y L(y, F(x)) P(y | x) dy$. For a given $x$ then, for the true underlying function, what is the probability of y given that x? This must be 1 when y is correct, and 0 otherwise. So this really just picks out $L(y, F(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc404a9-0870-451d-a12d-1e0f04e6f504",
   "metadata": {},
   "source": [
    "# 3 Finite data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247f6a1-8bf0-4671-991f-ef923fae0292",
   "metadata": {},
   "source": [
    "> <i>This nonparametric approach breaks down when the joint distribution of (y, x) is estimated by a finite data sample $\\{y_i, x_i\\}_i^N$. In this case $E_y[. | x]$ cannot be estimated accurately by its data value at each $x_i$, and even if it could, one would like to estimate $F^*(x)$ at x values other than the training sample points. Strength must be borrowed from nearby data points by imposing smoothness on the solution. One way to do this is to assume a parameterized form such as (2) and do parameter optimization as discussed in Section 1.1 to minimize the corresponding data based estimate of expected loss\n",
    "<br />\n",
    "<br />\n",
    "$\\{ \\beta_m, a_m\\}_i^M = \\text{argmin}_{\\beta_m, \\alpha_m} \\sum_{i=1}^N L \\big( y_i, \\sum_m \\beta_m h(x_i; a_m) \\big)$\n",
    "</i>\n",
    "\n",
    "This means we are back to assuming a parameterized model! And the model assumed here, is an additive one. Also, this optimization is done jointly--which gets harder and harder to do as the number of terms grows and the model complexity grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525c29f-2717-453d-a178-a0d902101115",
   "metadata": {},
   "source": [
    "> <i>In situations where this is infeasible one can try a \"greedy-stagewise\" approach. For $m = 1, 2, ..., M$\n",
    "<br />\n",
    "$(\\beta_m, \\alpha_m) = argmin_{\\beta, a} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\beta h(x_i; a)$\n",
    "<br />\n",
    "and then\n",
    "<br />\n",
    "$F_m(x) = F_{m - 1}(x) + \\beta_m h(x; a_m)$</i>\n",
    "\n",
    "So the tradeoff here is that we won't in general, obtain the correct solution. By freezing the set of parameters at each 'stage', this means we only have to learn the the weight and parameters of the given booster one at a time. It does make me wonder how much of a tradeoff this is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8bb24e-0fed-4cb9-bdc1-c9721424e5f6",
   "metadata": {},
   "source": [
    "> <i>Note that this stagewise strategy is different from stepwise approaches that readjust previously entered terms when new ones are added. In signal processing this stagewise strategy is called \"matching pursuit\" where L(y, F) is squared-error loss and the $\\{h(x; a_m\\}_1^M$ are called basis functions, usually taken from an over-complete wavelet-like dictionary. In machine learning, (9) (10) is called \"boosting\" where $y \\in \\{-1, 1\\}$ and $L(y, F)$ is either an exponential loss criterion $e^{-yF}$ or negative binomial log-likelihood. The function h(x; a) is called a \"weak learner\" or \"base learner\", and is usually a classification tree.</i>\n",
    "\n",
    "The \"matching pursuit\" algorithms arises from the paper <a href=\"https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf\">Matching Pursuits with Time-Frequency Dictionaries</a>. Uforunately, not much more I can say about it at this time.\n",
    "\n",
    "I have not studied much on the topic of sparse coding. But here's what I could gather. A vector is represented by a linear combination of a set of basis vectors (in general, no requirement these be orthogonal). When we say basis, what we mean is that each vector is UNIQUELY represented by a set of components (the coefficients in the linear sum of basis vectors). But it seems while mathematically nice, it's not always practical. It turns having the ability to represent a vector in more than one way brings benefits (I cannot articulate why at this time). This means we will need to more atoms than we need. This is where the term \"over-complete dictionary\" comes in. The wavelet refers to the type of $h(x; a)$ model we assume. The exponential loss is referring back to the Adaboost paper. Also, $h(x; a)$ is also called a \"booster\" as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fe3508-4bc5-4bcb-b443-f1088e6305a6",
   "metadata": {},
   "source": [
    "><i>Suppose for that for a particular loss $L(y, F)$ and/or base learner $h(x; a)$ the solution to $(\\beta_m, \\alpha_m) = argmin_{\\beta, a} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\beta h(x_i; a)$ is difficult to obtain. Given any approximator $F_{m-1}(x)$, the function $\\beta_m h(x; a_m)$ can be viewed the best greedy step towards the data based estimate of $F^*(x)$, under the constraint that the step \"direction\" $h(x; a_m)$ be a member of the parameterized class of functions $h(x;a)$. It can thus be regarded as a steepest-descent step under that constraint. By construction, the data based analogue of the unconstrained negative gradient\n",
    "<br/>\n",
    "<br/>\n",
    "$-g_m(x_i) = - \\big[ \\frac{\\partial L(y_i, F(x_i)}{\\partial F(x_i)} \\big]_{F(x) = F_{m - 1}(x)}$\n",
    "<br/>\n",
    "<br/>\n",
    "gives the best steepest-descent step direction $-g_m = \\{ -g_m(x_i)\\}_1^N$ in the N-dimensional data space at $F_{m-1}(x)$\n",
    "</i>\n",
    "\n",
    "What this is saying, is that we create a vector of made up N components, where a single component is $L(y_i, \\hat{y_i})$. By taking the gradient of this vector, and taking a step in the opposite direction of this gradient, we are approaching a point in this space where the loss becomes a minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e383c9-fd2c-4db5-a8a8-223142e35ed8",
   "metadata": {},
   "source": [
    "> <i>However, this gradient is only defined only at the data points $\\{ x_i\\}_i^N$ and cannot be generalized to other x-values. One possibility for generalization is to choose that member of the parameterized class $h(x; a_m)$ that produces $h_m = \\{ h(x_i; a_m \\}_{1}^N$ most parallel to $-g_m \\in R^N$. This is the $h(x; a)$ most highly correlated with $-g_m(x)$ over the data distribution. It can be obtained from the solution:\n",
    "<br />\n",
    "<br />\n",
    "$a_m = \\text{argmin}_{a, \\beta} \\sum_i^N [-g_m(x) - \\beta h(x_i; a)]^2$\n",
    "</i>\n",
    "\n",
    "Realize, we are back to a parameterized model. But what's novel here, is that are trying to learn the gradient of the loss function. The $h(x_i; a)$ is now a model that is trying to replicate the loss gradient. So this is a different type of learning. We can call this \"gradient learning\".\n",
    "\n",
    "Now, because of this, a new and different cost function is introduced. Here, the author chooses least squares (but of course other cost functions can be used). By learning the gradient, this allows the model to be defined for $x$ not in the training set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a426ffc-5bf8-4930-8c0c-1d9a52fa19be",
   "metadata": {},
   "source": [
    "> <i>This constrained negative gradient $h(x; a_m)$ is used in place of the unconstrained one in the steepest-descent strategy. Specifically, the line search is performed\n",
    "<br/>\n",
    "<br/>\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, F_{m - 1}(x_i) - \\rho h(x_i; a_m))$\n",
    "<br/>\n",
    "<br/>\n",
    "and the approximation updated\n",
    "<br/>\n",
    "<br/>\n",
    "$F_m(x) = F_{m-1}(x) + \\rho_m h(x; a_m)$\n",
    "<br/>\n",
    "<br/>\n",
    "Basically, instead of obtaining the solution under a smoothness constraint, the constraint is applied to the unconstrained (rough) solution by fitting $h(x; a)$ to the \"pseudo-responses\" $\\{ \\tilde{y_i} = -g_m(x_i)\\}_{i=1}^N$. This permits the replacement for the difficult function minimization problem by least-squares function minimization, followed by only a single parameter optimization based on the original criterion. Thus, for any $h(x; a)$ for this a feasible least-squares algorithm exists for solving $a_m = \\text{argmin}_{a, \\beta} \\sum_i^N [-g_m(x) - \\beta h(x_i; a)]^2$, one can use this approach to minimize any differential loss $L(y, F)$ in conjunction with forward stagewise additive modeling.\n",
    "    \n",
    "The uncontrained so-called \"rough\" solution is using the training gradient values (the value from calculating the deriative of the loss function). These values are what Friedman calls the \"pseudo-responses\". I mean, it's not the response value itself (which is $y$). But it's obtained after feeding our $y$ into the loss function and taking the first derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed57fd7-8046-41aa-ae04-49cb29186322",
   "metadata": {},
   "source": [
    "><i>This leads to the following (generic) algorithm using steepest-descent.\n",
    "<br />\n",
    "$F_0(x) = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, \\rho)$\n",
    "<br />\n",
    "For $m=1$ to M do:\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $y_i = - \\big[ \\frac{\\partial L(y_i, F(x_i)}{\\partial F(x_i)} \\big]_{F(x) = F_{m-1}(x)}$, over $i=1, N$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $a_m = \\text{argmin}_{a, \\beta} \\sum_i^N [\\tilde{y_i} - \\beta h(x_i; a)]^2$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\rho h(x_i; a_m))$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $F_m(x) = F_{m - 1}(x) + \\rho_m h(x; a_m)$\n",
    "<br />\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8978fd1-7dac-4390-aa06-dbef7d3239f2",
   "metadata": {},
   "source": [
    "💻 Let's implement this now. We will calculate the gradients manually. But to support general loss functions, we would need to rely on a package like <a href=\"https://jax.readthedocs.io/en/latest/index.html#\">jax</a>.\n",
    "\n",
    "Our first step is to get some data to try our algorithm on. Let's leverage sklearn for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7053b51-45e3-4fd0-9d18-92fea6d06a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sklearn.datasets.fetch_california_housing(return_X_y=False, download_if_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f059a31f-7480-490f-b642-69095534d689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Median house value in units of $100,000\n",
    "y = ds.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f47438ce-6c54-464b-8e80-95a901db28b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MedInc',\n",
       " 'HouseAge',\n",
       " 'AveRooms',\n",
       " 'AveBedrms',\n",
       " 'Population',\n",
       " 'AveOccup',\n",
       " 'Latitude',\n",
       " 'Longitude']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will ignore lat and long for our purposes\n",
    "# MedInc     = median income in block group\n",
    "# HouseAge   = median house age in block group\n",
    "# AveRooms   = average number of rooms per household\n",
    "# AveBedrms  = average number of bedrooms per household\n",
    "# Population = block group population\n",
    "# AveOccup   = average number of household members\n",
    "# Latitude   = block group latitude\n",
    "# Longitude  = block group longitude\n",
    "X = ds.data[:,0:-2]\n",
    "ds.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf2b1ea6-368a-4412-a4b4-33f62f179622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train=16512\n",
      "n_test=4128\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y,\n",
    "                                                                            test_size=.2,\n",
    "                                                                            random_state=random_state)\n",
    "\n",
    "X_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "y_scaler = preprocessing.StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "y_train = y_scaler.transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test = y_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f'n_train={X_train.shape[0]}')\n",
    "print(f'n_test={X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "479f2635-16f9-43ae-b289-e998a16eb2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.9999999999999996\n",
      "Test MSE: 0.9700794150600993\n"
     ]
    }
   ],
   "source": [
    "# For a baseline, let's use an average as the model\n",
    "train_error = mean_squared_error(y_train, [np.mean(y_train)] * len(y_train))\n",
    "test_error = mean_squared_error(y_test, [np.mean(y_train)] * len(y_test))\n",
    "print(f'Train MSE: {train_error}')\n",
    "print(f'Test MSE: {test_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57bc831b-b623-4433-a56f-2debf0e2eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.551537250945421\n",
      "Test MSE: 0.5434082038635916\n"
     ]
    }
   ],
   "source": [
    "# Compare to a single decision tree\n",
    "# We can of course, play with different max depths\n",
    "# Not setting max depth will immediately cause\n",
    "# the model to overfit. Try it for yourself.\n",
    "decision_tree_baseline = DecisionTreeRegressor(max_depth=2)\n",
    "decision_tree_baseline.fit(X_train, y_train)\n",
    "train_error = mean_squared_error(y_train, decision_tree_baseline.predict(X_train))\n",
    "test_error = mean_squared_error(y_test, decision_tree_baseline.predict(X_test))\n",
    "print(f'Train MSE: {train_error}')\n",
    "print(f'Test MSE: {test_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745fff3-b897-45a6-9a65-87408db59f8f",
   "metadata": {},
   "source": [
    "It's a little bit better. Let's try linear regression next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d03415e-59a5-4ecb-86ef-81e3842d9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.4635363995159715\n",
      "Test MSE: 0.4338923167239294\n"
     ]
    }
   ],
   "source": [
    "# Let's see what standard linear gression gives us\n",
    "model_baseline = LinearRegression()\n",
    "model_baseline.fit(X_train, y_train)\n",
    "train_error = mean_squared_error(y_train, model_baseline.predict(X_train))\n",
    "test_error = mean_squared_error(y_test, model_baseline.predict(X_test))\n",
    "print(f'Train MSE: {train_error}')\n",
    "print(f'Test MSE: {test_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e152184-e4df-4739-9d33-c70f03a818ae",
   "metadata": {},
   "source": [
    "Now we implement GBMs. We will end up implementing quite a few loss functions so we want to design a API to deal with all of this. We will make a GBM class that is a parent class and inherit base functionality from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30eadc35-bb47-4398-aeaf-650869e4cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquaresGBM():\n",
    "    def __init__(self,\n",
    "                 booster,\n",
    "                 n_boost,\n",
    "                 step_size=0.001,\n",
    "                 X_test=None,\n",
    "                 y_test=None):\n",
    "        \n",
    "        self.n_boost = n_boost\n",
    "        self.step_size = step_size\n",
    "        self.booster = booster\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.loss_fn = lambda y, y_hat: (y - y_hat)**2 / 2.\n",
    "        self.grad_loss_fn = lambda y, y_hat: -(y - y_hat)\n",
    "        \n",
    "        # Set during fit\n",
    "        # But set here for visibility sake\n",
    "        self.prior = None\n",
    "        self.boosters = []\n",
    "        self.training_loss = []\n",
    "        self.test_loss = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        self.prior = np.mean(y)\n",
    "\n",
    "        y_hat = np.array([self.prior] * len(y))\n",
    "        \n",
    "        self.training_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        for _ in range(self.n_boost):\n",
    "            \n",
    "            # Clone a model\n",
    "            model = clone(self.booster)\n",
    "            \n",
    "            # Calculate negative gradient values\n",
    "            y_train = self.grad_loss_fn(y, y_hat)\n",
    "                        \n",
    "            # Build a model that learns\n",
    "            # the negative of the gradient\n",
    "            model.fit(X, y_train)\n",
    "            \n",
    "            # Predict gradient values\n",
    "            grad_hat = model.predict(X)\n",
    "            \n",
    "            # Take a step so that loss function\n",
    "            # decreases\n",
    "            y_hat = y_hat - self.step_size * grad_hat\n",
    "            \n",
    "            # Save model for future inferences\n",
    "            self.boosters.append(model)\n",
    "            \n",
    "            # Predict what we have thus far\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # Compute loss at each step\n",
    "            train_loss = np.mean(2 * self.loss_fn(y, y_pred))\n",
    "            \n",
    "            self.training_loss.append(train_loss)\n",
    "            \n",
    "            if self.X_test is not None:\n",
    "                y_pred = self.predict(self.X_test)\n",
    "                test_loss = np.mean(2 * self.loss_fn(self.y_test, y_pred))\n",
    "                self.test_loss.append(test_loss)\n",
    "                \n",
    "        self.training_time = time.time() - t0\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        y_hat = np.array([self.prior] * X.shape[0])\n",
    "        \n",
    "        for booster in self.boosters:\n",
    "            \n",
    "            # Make a prediction\n",
    "            grad_hat = booster.predict(X)\n",
    "            \n",
    "            # Take step\n",
    "            y_hat = y_hat - self.step_size * grad_hat\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "474f30b2-2c13-49cb-a425-431521450713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:    0.3538803967189855\n",
      "Test MSE:     0.3485243625594113\n",
      "Traing Time:  1.6985301971435547 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x121834af0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LeastSquaresGBM(booster=DecisionTreeRegressor(max_depth=2, criterion='squared_error'),\n",
    "                        n_boost=50,\n",
    "                        step_size=.1,\n",
    "                        X_test=X_test,\n",
    "                        y_test=y_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_error = mean_squared_error(y_train, model.predict(X_train))\n",
    "test_error = mean_squared_error(y_test, model.predict(X_test))\n",
    "\n",
    "print(f'Train MSE:    {train_error}')\n",
    "print(f'Test MSE:     {test_error}')\n",
    "print(f'Traing Time:  {model.training_time} seconds')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(model.training_loss)), model.training_loss, marker='o', color='gray', label='train')\n",
    "ax.plot(range(len(model.test_loss)), model.test_loss, marker='o', color='red', label='test')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_xlabel('Number of Rounds')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6602a-8d1e-4c6f-b42b-73e18a5bbc0b",
   "metadata": {},
   "source": [
    "> <i>Note that any fitting criterion that estimates conditional expecation (given x) could in principle be used to estimate the (smoothed) negative gradient at line 4 of Algorithm 1. Least-squares (11) is a natural choice owing to the superior computational properties of many least-squares algorithm\n",
    "\n",
    "Yes. It's really just learning the gradient of the loss. So any cost function will work. Least-squares is a reasonable default IMHO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f791e20-5313-451c-a51e-8b115c8cdb5a",
   "metadata": {},
   "source": [
    "><i> In the special case where $y \\in \\{-1, 1\\}$ and the loss function $L(y, F)$ depends on y and F only through their product $L(y, F) = L(yF)$, the analogy of boosting (9), (10) to steepest-descent minimization has been noted in the machine learning literature. Duffy and Hemlbold elegantly exploit this analogy to motivate their GeoLev and and GeoArc procedures. The quantity $yF$ is call the \"margin\" and the steepest-descent is performed in the space of margin values, rather than the space of function values F. The latter approach permits application to more general loss functions where the notion of margins is not apparent. Durker employes a different strategy of casting regression in the framework of classification in the context of the AdaBoost algorithm.\n",
    "    \n",
    "It seems they make aware they are not the first to generalize AdaBoost. AdaBoost was generalized to margin values but a generalization to optimizing over function space direclty is indeed, the most general. It's a nice framework, because it leads to new classes of algorithms.\n",
    "    \n",
    "I never read the paper behind GeoLev and GeoArc but it can be found here: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.4750&rep=rep1&type=pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5cb9a-a357-49ea-b04d-ef6146fca156",
   "metadata": {},
   "source": [
    "# 4. Applications: additive modeling\n",
    "\n",
    "> <i>In this section the gradient boosting strategy is applied to several popular loss critiera: least-squares (LS), least absolute devation (LAD), Huber (M), and logistic binomial log-likelihood (L). The first serves as a \"reality check\", whereas the others lead to new boosting algorithms.\n",
    "    \n",
    "These are all the cost functions we coded up from the abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3697c9ca-42f3-4f0e-a227-19773f991a2a",
   "metadata": {},
   "source": [
    "## 4.1 Least-square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef771583-abcc-4b56-b12d-1018d24635f9",
   "metadata": {},
   "source": [
    "> <i>4.1 Least-squares regression. Here $L(y, F) = \\frac{1}{2}(y - F)^2$. The pseudoresponse in line 3 of algorithm is $\\tilde{y_i} = y_i - F_{m - 1}(x_i)$\n",
    "    \n",
    "Yes, $-[\\frac{\\partial L(y, F)}{\\partial F}] = y - F$ (use chain rule to solve). So we fitting a model where our target is the residual. If the residual is positive, we are underestimating the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc0a3f-d004-48a7-9ac7-de4592c61539",
   "metadata": {},
   "source": [
    "><i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1ea89-95ae-4d5e-aa89-02648c6f6b2d",
   "metadata": {},
   "source": [
    "> <i>Thus, line 4 simply fits the current residuals and the line search (line 5) produces the result $\\rho_m = \\beta_m$, where $\\beta_m$ is the minimizing $\\beta$ of line 4. Therefore, gradient boosting on squared-error loss produces the usual stagewise approach of iteratively fitting the current residuals.\n",
    "<br/>\n",
    "<br/>\n",
    "ALGORITHM 2 (LS_Boost)\n",
    "<br/>\n",
    "$F_0(x) = \\bar{y}$ [take average]\n",
    "<br/>\n",
    "For $m=1$ to M do:\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $\\tilde{y_i} = y_i - F_{m-1}(x_i)$, $i=1, N$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $(a_m, \\rho_m) = \\text{argmin}_{a, \\rho} \\sum_i^N [\\tilde{y_i} - \\rho h(x_i; a)]^2$\n",
    "<br />\n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  $F_m(x) = F_{m - 1}(x) + \\rho_m h(x; a_m)$\n",
    "<br />\n",
    "</i>\n",
    "\n",
    "So line (5) referenced is this:\n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N L(y_i, F_{m-1}(x_i) + \\rho h(x_i; a_m))$\n",
    "\n",
    "Since the loss is least-squares we get:\n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N (y_i - (F_{m-1}(x_i) + \\rho h(x_i; a_m)))^2$\n",
    "\n",
    "rearranging:\n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N (y_i - (F_{m-1}(x_i)) - \\rho h(x_i; a_m)))^2$\n",
    "\n",
    "and since $y_i - F_{m-1}(x_i) = \\tilde{y_i}$ \n",
    "\n",
    "$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N [\\tilde{y_i} - \\rho h(x_i; a_m)]^2$\n",
    "\n",
    "That's why the same equation gets repeated twice. Once for $\\rho_m$ and another time for $\\alpha_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bcb3b9-7220-4be2-98b1-949467469ca4",
   "metadata": {},
   "source": [
    "## 4.2 Least absolute deviation (LAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815d691-4576-4c0d-80bc-64508ef89644",
   "metadata": {},
   "source": [
    "><i> For the loss function L(y, F) = |y - F|, one has\n",
    ">\n",
    ">$\\tilde{y_i} = -[\\frac{\\partial L(y_i, F(x_i)}{\\partial F(x_i)}]_{F(x) = F_{m-1}(x)} = \\text{sign}(y_i - F_{m-1}(x_i))$ \n",
    "\n",
    "This can be shown as follows. We can break the loss into a conditional statement:\n",
    "    \n",
    "$L(y_i, F(x_i)) = y_i - F(x_i)$ [for $y_i > F(x_i)$]\n",
    "    \n",
    "$L(y_i, F(x_i)) = F(x_i) - y_i$ [for $F(x_i) > y_i$]\n",
    "    \n",
    "$L(y_i, F(x_i)) = 0$ [for $F(x_i) = y_i$]\n",
    "    \n",
    "Taking the partial deriative piecewise we get:\n",
    "    \n",
    "$-1$ [for $y_i > F(x_i)$]\n",
    "    \n",
    "$+1$ [for $F(x_i) > y_i$]\n",
    "    \n",
    "$\\text{undefined}$ [for $F(x_i) = y_i$]\n",
    "    \n",
    "This is equivalent to $- \\text{sign}(y_i - F(x_i))$. (Notice the minus in front!). Remember, they are learning the negative of the gradient. So when the step is taken, you add the step (not subtract). Also, notice we are learning the sign of the error. And that even if the error is large, its magnitude is ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9ac2d-5699-4e81-989f-3c15790592b5",
   "metadata": {},
   "source": [
    "> <i>This implies that $h$ is fit (least-squares) to the sign of the current residuals in > line 4 of Algorithm 1. The line search (line 5) becomes\n",
    ">\n",
    ">$\\rho_m = \\text{argmin}_{\\rho} \\sum_i^N |y_i - F_{m-1}(x_i) - \\rho h(x_i; a_m)|$\n",
    ">$= \\text{argmin}_{\\rho} \\sum_i^N |h(x_i; a_m)| |\\frac{y_i - F_{m-1}(x_i)}{h(x_i; a_m)} - \\rho|$\n",
    "><br />\n",
    ">$= \\text{median}_w \\{\\frac{y_i - F_{m-1}(x_i)}{h(x_i; a_m)}\\}$ &nbsp;&nbsp;&nbsp; $w_i = |h(x_i; a_m)|$\n",
    "    \n",
    "The key to understanding the solution to this, is to see this <a href=\"https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-ell-1-norm\">answer</a>. That is, the solution to the following:\n",
    "    \n",
    "$\\sum_i^N |x_i - \\alpha|$\n",
    "\n",
    "If $\\alpha$ is the median, then we are able to minimize the quantity. But for our case, is that each number in the series is weighted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891ca3f-cffa-4648-a44a-e0a769964076",
   "metadata": {},
   "source": [
    "><i> Here median_W{.} is the weighted median with weights $w_i$. Inserting these results into Algorithm 1 yields an algorithm for least absolute deviation boosting, using any base learner $h(x|a)$\n",
    "    \n",
    "Nice thing about this, is that the optimal learning rate is able to be computed at each step.\n",
    "    \n",
    "💻 Let's implement this now. We could of course do better with abstractions here, but we will wait to do a proper abstraction at the very end in order to make everything as explicit as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "333d967e-e628-49ca-ac31-f863334f9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastAbsDeviationGBM():\n",
    "    def __init__(self,\n",
    "                 booster,\n",
    "                 n_boost,\n",
    "                 X_test=None,\n",
    "                 y_test=None):\n",
    "        \n",
    "        self.n_boost = n_boost\n",
    "        self.booster = booster\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.loss_fn = lambda y, y_hat: np.abs(y - y_hat)\n",
    "        self.grad_loss_fn = lambda y, y_hat: -np.sign(y - y_hat)\n",
    "        \n",
    "        # Set during fit\n",
    "        # But set here for visibility sake\n",
    "        self.prior = None\n",
    "        self.boosters = []\n",
    "        self.training_loss = []\n",
    "        self.test_loss = []\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        self.prior = np.mean(y)\n",
    "\n",
    "        y_hat = np.array([self.prior] * len(y))\n",
    "        \n",
    "        self.training_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        for _ in range(self.n_boost):\n",
    "            \n",
    "            # Clone a model\n",
    "            model = clone(self.booster)\n",
    "            \n",
    "            # Calculate negative gradient values\n",
    "            y_train = self.grad_loss_fn(y, y_hat)\n",
    "                        \n",
    "            # Build a model that learns\n",
    "            # the negative of the gradient\n",
    "            model.fit(X, y_train)\n",
    "            \n",
    "            # Predict gradient values\n",
    "            grad_hat = model.predict(X)\n",
    "            \n",
    "            # Compute optimal step size for this loss\n",
    "            w = np.abs(grad_hat)\n",
    "            step_size = np.median(w * ((y - y_hat) / grad_hat))\n",
    "                \n",
    "            # Take a step so that loss function decreases\n",
    "            y_hat = y_hat - step_size * grad_hat\n",
    "            \n",
    "            # Save model for future inferences\n",
    "            self.boosters.append(model)\n",
    "            \n",
    "            # Predict what we have thus far\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # Compute loss at each step\n",
    "            train_loss = np.mean(self.loss_fn(y, y_pred))\n",
    "            \n",
    "            self.training_loss.append(train_loss)\n",
    "            \n",
    "            if self.X_test is not None:\n",
    "                y_pred = self.predict(self.X_test)\n",
    "                test_loss = np.mean(self.loss_fn(self.y_test, y_pred))\n",
    "                self.test_loss.append(test_loss)\n",
    "                \n",
    "        self.training_time = time.time() - t0\n",
    "            \n",
    "    def predict(self, X, step_size=.01):\n",
    "        \n",
    "        y_hat = np.array([self.prior] * X.shape[0])\n",
    "        \n",
    "        for booster in self.boosters:\n",
    "            \n",
    "            # Make a prediction\n",
    "            grad_hat = booster.predict(X)\n",
    "            \n",
    "            # Take step\n",
    "            y_hat = y_hat - step_size * grad_hat\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3351516-832d-4a11-abd9-e7e95bc7cbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE:    0.761384080489687\n",
      "Test MSE:     0.731653980131292\n",
      "Traing Time:  1.7811100482940674 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12198dee0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LeastAbsDeviationGBM(booster=DecisionTreeRegressor(max_depth=2, criterion='squared_error'),\n",
    "                             n_boost=60,\n",
    "                             X_test=X_test,\n",
    "                             y_test=y_test)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_error = mean_squared_error(y_train, model.predict(X_train))\n",
    "test_error = mean_squared_error(y_test, model.predict(X_test))\n",
    "\n",
    "print(f'Train MSE:    {train_error}')\n",
    "print(f'Test MSE:     {test_error}')\n",
    "print(f'Traing Time:  {model.training_time} seconds')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(len(model.training_loss)), model.training_loss, marker='o', color='gray', label='train')\n",
    "ax.plot(range(len(model.test_loss)), model.test_loss, marker='o', color='red', label='test')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_xlabel('Number of Rounds')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dec8e9-c6ce-4fd2-9fea-9efe1d31605c",
   "metadata": {},
   "source": [
    "A few points to consider from the above implementation, is that we had to pick a constant step size (could still be dynamic) for the prediction step. But we can't calculate this without labeled data (the loss requires a label to evaluate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42138d99-282d-4088-8453-6bac7212df90",
   "metadata": {},
   "source": [
    "> Here we consider the special case where each base learner is an J-terminal node regression tree. Each regression tree model itself the has additive form:\n",
    ">\n",
    "> $h(x; \\{b_j, R_j\\}_1^J) = \\sum_j^J b_j1 (x \\in R_j)$\n",
    "\n",
    "We have already been doing this, we have been choosing our base learners to be CART (classification and regression trees) models. In fact, CART (classification and regression trees) were invented by Jerome Friedman. This <a href=\"https://www.youtube.com/watch?v=8hupHmBVvb0\">video</a> is short neat history by Friedman himself on how he invented CART. When we arrive at the last node with all these features, we take all the labels associated with the training examples in that region--and we take the average of them.\n",
    "\n",
    "What are the parameters of this model? Well it's a collection of the splitting variables and the split values themselves. We can calculate the information storage of this model. Let's say we have p features. Well, the number of distinct representations is given by:\n",
    "\n",
    "$2^b = p$ where $b$ is the number of bits we need and $p$ is all the possible features we might have. Solving:\n",
    "\n",
    "$b = log_2(p)$. Now let's store each split value as a float. This requires 32 bits. So for a given node, we have:\n",
    "\n",
    "$\\text{number of bits} = 32 * log_2(p)$\n",
    "\n",
    "But how many nodes? Assuming each branch leads to a total depth, we get: \n",
    "\n",
    "$\\text{bits(p, max_depth)} = 32 * log_2(p) * 2^{\\text{max_depth}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528ecbb-d012-4582-9620-89a568d8fab8",
   "metadata": {},
   "source": [
    "> <i>Here $\\{R_j\\}_1^J$ are disjoint regions that collectively cover the space of all joint values of the predictor variables $\\bf{x}$. These regions are represented by the terminal nodes of the corresponding tree. The indicator function 1(.) has the value 1 if its argument is true, and zero otherwise.. The \"paramaters\" of this base learner (15) are the coefficients $\\{b_j\\}_1^J$, and the quantities that define the boundaries of the regions $\\{R_j\\}_1^J$. These are the splitting variables and the values of those variables that represent the splits at the nonterminal nodes of the tree. Because the regions are disjoint, (15) is equivalent to the prediction rule: if $x \\in R_j$ then $h(x) = b_j$.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49716254-7d78-4c55-80d8-22a34d6d71bf",
   "metadata": {},
   "source": [
    "The tree creates non-overlapping regions in feature space. Each feature vector gets mapped to a distinct region. And the prediction will be an average all the labels in that region (as defined by the training data). So we will end up with an equation like:\n",
    "\n",
    "$F_m(x) = F_{m-1}(x) + \\rho_m \\sum_{j=1}^J b_{jm} 1(x \\in R_{jm})$\n",
    "\n",
    "This is when the author points our that $\\rho_m$ can be \"brought in\" to the node region itself, by making a new definition of $\\gamma_{jm} = \\rho_m b_{jm}$. So we get:\n",
    "\n",
    "$F_m(x) = F_{m-1}(x) + \\sum_{j=1}^J \\gamma_{jm} 1(x \\in R_{jm})$\n",
    "\n",
    "Now, we no longer have to learn a parameter value that must work across all the leafs. What is the optimal $\\gamma_{jm}$? at a given $m$? We consult our loss function to find it:\n",
    "\n",
    "$\\vec{\\gamma_{jm}} = \\text{argmin}_{\\gamma_{jm}} \\sum_i^N L \\big( y_i, F_{m-1}(x) + \\sum_{j=1}^J \\gamma_{jm} 1(x \\in R_{jm}) \\big)$\n",
    "\n",
    "We have to calculate $J$ different values in the optimization setup. But, a key insight here is that a given example maps to a unique leaf. Thus, we can break this up into $J$ independent optimization problems:\n",
    "\n",
    "$\\gamma_{jm} = \\text{argmin}_{\\gamma} \\sum_{i \\in R_{jm}} L \\big( y_i, F_{m-1}(x) + \\gamma \\big)$\n",
    "\n",
    "Suppose the loss is $|y - F(x)|$ (least absolute deviation). Recall the solution to minimize\n",
    "\n",
    "$J(\\alpha) = \\sum_i^N |x_i - \\alpha|$ is the median value of $x$.\n",
    "\n",
    "$\\gamma_{jm} = \\text{argmin}_{\\gamma} \\sum_{i \\in R_{jm}} |y_i - F_{m-1}(x) - \\gamma |$\n",
    "\n",
    "So the 'best' leaf value is calculating the residual of each ($y - \\hat{y}$) in each leaf and takings it median. To build the tree, a regression tree is used to predict the sign of the residual (i.e., we are building a model to predict the gradient) using least squares criterion. Sklearn provides a `DecisionTreeRegressor` class with a criterion of `squared_error`.\n",
    "\n",
    "It's interesting we build the tree using a squared_error objective but then take the median values in leafs. Why? The author states:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c421132-ae08-4120-bdc4-60f81cb816fb",
   "metadata": {},
   "source": [
    "> <i>Squared-error loss is much more rapidly updated than mean absolute deviation when searcing for splits during the tree building process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818f1b2-7955-4051-a6c9-f030b0515908",
   "metadata": {},
   "source": [
    "Why is this? There are clues from https://github.com/scikit-learn/scikit-learn/issues/9553 and https://github.com/scikit-learn/scikit-learn/issues/9626. I would need to dive deeper into this to understand exactly why it's so slow.\n",
    "\n",
    "\n",
    "Also, even better if we built the tree to minimize the loss criterion directly (instead of it using its own loss function). This is optimal, but the tradeoff Friedman claims is speed. I believe XGBoost solves this problem in fact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae556ef-e7f0-4bd9-9e41-23d5b73ca502",
   "metadata": {},
   "source": [
    "## 4.4 M-Regression\n",
    "\n",
    "Next, the author calculates the gradient for M-regression. The idea in M-regression, is to handle outliers explicitly. The idea seems to be as follows. When the absolute value of the residual is greater than some constant, switch to a different loss. So our loss uses a switch function. The lossed used for these outliers is:\n",
    "\n",
    "$L(y, \\hat{y}) = \\delta (|y - \\hat{y}| - \\delta/2)$\n",
    "\n",
    "Like I said before, my first exposure to M-regression was in this paper. But it's not hard to reason why this particular form is assumed. Let's calculate the loss at the critical point:\n",
    "\n",
    "$L(y, \\hat{y}) = \\delta (\\delta - \\delta/2) = \\delta^2/2$\n",
    "\n",
    "At the critical point, the loss is no different than had we used squared error loss, this is probably why it takes the form it does, so the loss remains a continous function. Let's actually plot the loss as a function of residual for original squared error loss and the M-regression loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "994c2d57-7f8d-440d-8a3f-a18d324c7e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1229935b0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "delta = .2\n",
    "residual = np.linspace(start=0, stop=1, num=10)\n",
    "squared_error_loss = lambda x: .5 * x**2\n",
    "huber_error_loss = lambda x: .5 * x**2\n",
    "\n",
    "def huber_loss(residual, delta):\n",
    "    \n",
    "    # Use original loss\n",
    "    loss = squared_error_loss(residual)\n",
    "    \n",
    "    # Now correct outliers\n",
    "    I = residual > delta\n",
    "    huber_loss = delta * (residual - delta/2.)\n",
    "    loss[I] = huber_loss[I]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "ax.set_title('Huber Loss')\n",
    "ax.set_xlabel('|Residual|')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('|y - $\\hat{y}$|')\n",
    "ax.axvline(x=delta, linestyle='--', alpha=.3, color='black')\n",
    "ax.plot(residual, squared_error_loss(residual), marker='o', color='gray', label='squared error')\n",
    "ax.plot(residual, huber_loss(residual, delta), marker='o', color='blue', label='huber')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2b649-a757-4609-ba2c-f8274ea36670",
   "metadata": {},
   "source": [
    "The graphs diverge (as expected), but the Huber loss is clearly less sensitive to outliers. But here's a question: how do we pick delta? There's a simple hueristic mentioned, which is simply choosing a quantile in the distribution of absolute value of the residuals. So my guess would be to take all values above the 99% percentile to be outlier like for example. But, I think (although the authors don't mention this explicitly), you can treat this value as a hyperparameter.\n",
    "\n",
    "OK, next we need to calculate the gradient. The loss function is:\n",
    "\n",
    "if $|y - f| \\le \\delta$ then:\n",
    "\n",
    "$l(y, f) =  \\frac{1}{2} (y - f)^2$\n",
    "\n",
    "else:\n",
    "\n",
    "$l(y, f) =  \\delta(|y - f| - \\frac{\\delta}{2})$\n",
    "\n",
    "So the gradient will be:\n",
    "\n",
    "if $|y - f| \\le \\delta$ then:\n",
    "\n",
    "$y - f$\n",
    "\n",
    "else:\n",
    "\n",
    "$\\delta \\; \\text{sign}(y - \\hat{y})$ \n",
    "\n",
    "We had derived the gradient for the absolute deviation loss earlier, and so we simply add the $\\delta$ constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62c3a-0f07-463b-bd28-440dc7295519",
   "metadata": {},
   "source": [
    "## 4.5 Two-class logistic regression and classification\n",
    "\n",
    "For this loss, the author introduces the idea of \"influence\" trimming. What this means, is under this loss, there are some training examples that will basically add nothing to our loss. So they might as well be dropped. This offers some computational benefits. For example, if on a single machine with examples that can't be fit into memory, this means we might be able to only the load the batch of data that will have influence.\n",
    "\n",
    "But what I find more fascinating with influence trimming is more a fundamental question. How much of the training data do we actually need? That is, for the model and given hyperparameters, there's some minimimal amount of training examples needed to acheive the same quality of fit. In fact, Freidman mentions the concept of \"weight trimming\" in Real AdaBoost, which was able to delete 95% of examples without compromising quality. That's crazy. I would love to learn more about this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
