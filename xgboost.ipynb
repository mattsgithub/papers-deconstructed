{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfb765c-2402-4275-b986-946e2f9fac18",
   "metadata": {},
   "source": [
    "# PAPERS DECONSTRUCTED\n",
    "## Title: XGBoost: A Scalable Tree Boosting System\n",
    "### Authors: Tianqi Chen, Carlos Guestrin\n",
    "Link: https://arxiv.org/abs/1603.02754"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341804b5-f8ff-4b2c-82e9-2aa7ec1f6193",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "The core idea is to take the original <a href=\"https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full\">work</a> of Friedman for boosting algorithms with the following improvements:\n",
    "\n",
    "(1) Add regularization into the framework explicitly\n",
    "\n",
    "(2) Add a bunch of computer science optimiations:\n",
    "\n",
    "    - Make finding split points more efficient\n",
    "    - Make sparsity a first-class citizen\n",
    "    - Improve how sorting a feature is done\n",
    "    - Caching access of gradient statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb1814-d79d-4be5-bdb2-ed58c7cfccee",
   "metadata": {},
   "source": [
    "# Core Algorithm\n",
    "\n",
    "The idea is to use a Taylor series approximation of the loss function and just work off of that. For a single valued function, a Taylor series looks like:\n",
    "\n",
    "$f(x) = f(x_0) + f'(x_0)(x - x_0) + \\frac{1}{2}f''(x_0)(x - x_0)^2$\n",
    "\n",
    "In our case, the loss function is a function of two values: $l(y, \\hat{y})$. In boosting, $\\hat{y}$ is a sum of all previous boosts:\n",
    "\n",
    "$\\hat{y}_m = f_m(x) + F_{m-1}(x)$\n",
    "\n",
    "Where $F_{m-1}(x) = f_{m-1}(x) + f_{m-2}(x) + \\dots$\n",
    "\n",
    "Let's look at our loss:\n",
    "\n",
    "$J(y, \\hat{y}) = \\sum_i l(y, \\hat{y})$\n",
    "\n",
    "But the authors also make the point that they want to add some reguarlization to each boost. So we get:\n",
    "\n",
    "$J(y, f_m(x) + F_{m-1}(x)) = \\sum_i^N l(y, \\hat{y}) + \\sum_k^{m} \\Omega(f_k(x))$\n",
    "\n",
    "So it's assumed each example is independent. Now, we want to write a Taylor series approximation of the loss function. So let's focus on:\n",
    "\n",
    "$l(y, \\hat{y}) = l(y, f_m(x) + F_{m-1}(x))$\n",
    "\n",
    "If we assume a stage-wise approach (where all previous terms cannot be modified). Then $f_m(x)$ is like our $x$ variable we want to expand over just like in the one-variable Taylor series. So we get: \n",
    "\n",
    "$l(y, f_m(x) + F_{m-1}(x)) \\approx l(y, F_{m-1}(x)) + \\frac{\\partial l}{\\partial f}_{f = F_{m-1}(x)} f_m(x) + \\frac{1}{2} \\frac{\\partial^2 l}{\\partial f^2} (f_m(x))^2$\n",
    "\n",
    "Plugging this back into the main equation:\n",
    "\n",
    "$J(y, \\hat{y}) = \\sum_i^N \\big( l(y_i, F_{m-1}(x_i)) + \\frac{\\partial l}{\\partial f}_{f = F_{m-1}(x_i)} f_m(x_i) + \\frac{1}{2} \\frac{\\partial^2 l}{\\partial f^2} (f_m(x))^2 \\big) + \\sum_k^{m} \\Omega(f_k(x))$\n",
    "\n",
    "We want to find a $f_m(x_i)$ such that it minimizes this cost function. This means off the bat, we can ignore $l(y_i, F_{m-1}(x_i))$\n",
    "\n",
    "So now we get:\n",
    "\n",
    "$\\tilde{J}(y, \\hat{y}) = \\sum_i^N \\big( \\frac{\\partial l}{\\partial f}_{f = F_{m-1}(x_i)} f_m(x_i) + \\frac{1}{2} \\frac{\\partial^2 l}{\\partial f^2} (f_m(x))^2 \\big) + \\sum_k^{m} \\Omega(f_k(x))$\n",
    "\n",
    "$f_m(x)$ takes an example and directs it to a leaf (assuming CART is our booster). This mapping is unique. So we can think of this as a vector of leafs. This vector will have zero components everwhere except at the index $x$ maps to. It's length is the number of leafs.\n",
    "\n",
    "So we have $f_m(x_i) = \\vec{w_j}(x_i)$\n",
    "\n",
    "The trick here is that we always take the non-zero component.\n",
    "\n",
    "So our entire example space can be partitioned into the leaves that will map to. So we can add a new summation term that sums the examples by leaf. Let's say we have T leafs:\n",
    "\n",
    "\n",
    "$\\tilde{J} = \\sum_j^T \\sum_{i \\in R_j} \\big( \\frac{\\partial l}{\\partial f}_{f = F_{m-1}(x_i)} \\vec{w_j} + \\frac{1}{2} \\frac{\\partial^2 l}{\\partial f^2} (\\vec{w_j})^2 \\big) + \\sum_j^{T} \\Omega(f_{j}(x))$\n",
    "\n",
    "The last term can be simplifed since all previous trees are frozen (i.e., they are constants). This means $\\Omega(f_{jk}(x)) = \\Omega(f_m(x))$\n",
    "\n",
    "Now, how exactly do we want to quantify the regularaization? The authors propose that the number of leafs and their values are the things we should control from unregulated growth. \n",
    "\n",
    "So $\\Omega(f_m(x)) = \\gamma T + \\frac{1}{2} \\lambda \\sum_j \\vec{w}^2_j$\n",
    "\n",
    "Plugging this all back in:\n",
    "\n",
    "\n",
    "$\\tilde{J} = \\sum_j^T \\sum_{i \\in R_j} \\big( \\frac{\\partial l}{\\partial f}_{f = F_{m-1}(x_i)} \\vec{w_j} + \\frac{1}{2} \\frac{\\partial^2 l}{\\partial f^2} \\vec{w_j}^2 \\big) +  \\gamma T + \\frac{1}{2} \\lambda \\sum_j \\vec{w}^2_j$\n",
    "\n",
    "We can rearange sum:\n",
    "\n",
    "$\\tilde{J} = \\sum_j^T \\big(  \\vec{w_j} \\sum_{i \\in R_j} \\frac{\\partial l}{\\partial f}_{f = F_{m-1}(x_i)} + \\frac{1}{2} \\vec{w_j}^2 \\sum_{i \\in R_j} \\frac{\\partial^2 l}{\\partial f^2} +  \\frac{1}{2} \\lambda \\vec{w}^2_j \\big) +  \\gamma T$\n",
    "\n",
    "We can make some defitions to clean things up:\n",
    "\n",
    "$G_i = \\sum_{i \\in R_j} \\frac{\\partial l}{\\partial f}_{f = F_{m-1}(x_i)}$\n",
    "\n",
    "$H_i = \\sum_{i \\in R_j} \\frac{\\partial^2 l}{\\partial f^2}_{f = F_{m-1}(x_i)}$\n",
    "\n",
    "\n",
    "$\\tilde{J} = \\sum_j^T \\big(  \\vec{w_j} G_i + \\frac{1}{2} \\vec{w_j}^2 H_i +  \\frac{1}{2} \\lambda \\vec{w}^2_j \\big) +  \\gamma T$\n",
    "\n",
    "Things are starting to look better!\n",
    "\n",
    "$\\tilde{J} = \\sum_j^T \\big(  \\vec{w_j} G_i + \\frac{1}{2} \\vec{w_j}^2 (H_i + \\lambda) \\big) +  \\gamma T$\n",
    "\n",
    "To find the lowest, for a fixed set of leafs, we need to minimize:\n",
    "\n",
    "$\\sum_j^T \\big(  \\vec{w_j} G_i + \\frac{1}{2} \\vec{w_j}^2 (H_i + \\lambda) \\big)$\n",
    "\n",
    "Each leaf will not share examples with other leafs. So for each leaf, we want to make \n",
    "\n",
    "$\\vec{w_j} G_i + \\frac{1}{2} \\vec{w_j}^2 (H_i + \\lambda)$ as small as possible. Taking its derivative with respect to $\\vec{w_j}$ and setting to 0:\n",
    "\n",
    "$G_i + \\vec{w_j} (H_i + \\lambda) = 0$\n",
    "\n",
    "Now, we can solve for $\\vec{w_j}$\n",
    "\n",
    "$\\vec{w_j} = -\\frac{G_i}{H_i + \\lambda}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
